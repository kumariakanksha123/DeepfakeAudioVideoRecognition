{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import (Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D,\n",
    "                          BatchNormalization, Input, Conv2D, GlobalAveragePooling2D,concatenate,Concatenate,multiply, LocallyConnected2D, Lambda)\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import EfficientNetB7\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from PIL import Image, ImageOps\n",
    "import cv2\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "#from keras.applications.resnet50 import preprocess_input\n",
    "from keras.applications.densenet import DenseNet121,DenseNet169,preprocess_input\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score, fbeta_score, cohen_kappa_score\n",
    "\n",
    "import keras.callbacks as callbacks\n",
    "from keras.callbacks import Callback\n",
    "get_ipython().run_line_magic('config', 'InlineBackend.figure_format=\"svg\"')\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SIZE = 224\n",
    "NUM_CLASSES = 2\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Embedding, LSTM, Dropout, Dense, Input, Bidirectional, Flatten, Conv2D, MaxPooling2D, concatenate, Conv1D, MaxPooling1D\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten, concatenate\n",
    "from keras.models import Model\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 60.9 MB 2.8 MB/s eta 0:00:013   |███▋                            | 6.9 MB 393 kB/s eta 0:02:18     |██████▊                         | 12.7 MB 15.5 MB/s eta 0:00:04\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /home/arman/anaconda3/lib/python3.9/site-packages (from opencv-python) (1.23.0)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.6.0.66\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached https://files.pythonhosted.org/packages/31/9f/042db462417451e81035c3d43b722e88450c628a33dfda69777a801b0d40/scikit_learn-0.20.4-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting scipy>=0.13.3 (from scikit-learn)\n",
      "  Using cached https://files.pythonhosted.org/packages/24/40/11b12af7f322c1e20446c037c47344d89bab4922b8859419d82cf56d796d/scipy-1.2.3-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting numpy>=1.8.2 (from scikit-learn)\n",
      "  Using cached https://files.pythonhosted.org/packages/3a/5f/47e578b3ae79e2624e205445ab77a1848acdaa2929a00eeef6b16eaaeb20/numpy-1.16.6-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Installing collected packages: numpy, scipy, scikit-learn\n",
      "Successfully installed numpy-1.16.6 scikit-learn-0.20.4 scipy-1.2.3\n",
      "Collecting scikit-learn\n",
      "  Downloading https://files.pythonhosted.org/packages/f5/ef/bcd79e8d59250d6e8478eb1290dc6e05be42b3be8a86e3954146adbc171a/scikit_learn-0.24.2-cp36-cp36m-manylinux1_x86_64.whl (20.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 20.0MB 50kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy\n",
      "  Downloading https://files.pythonhosted.org/packages/c8/89/63171228d5ced148f5ced50305c89e8576ffc695a90b58fe5bb602b910c2/scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 25.9MB 46kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting matplotlib\n",
      "  Downloading https://files.pythonhosted.org/packages/09/03/b7b30fa81cb687d1178e085d0f01111ceaea3bf81f9330c937fb6f6c8ca0/matplotlib-3.3.4-cp36-cp36m-manylinux1_x86_64.whl (11.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 11.5MB 88kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib>=0.11 (from scikit-learn)\n",
      "  Downloading https://files.pythonhosted.org/packages/3e/d5/0163eb0cfa0b673aa4fe1cd3ea9d8a81ea0f32e50807b0c295871e4aab2e/joblib-1.1.0-py2.py3-none-any.whl (306kB)\n",
      "\u001b[K    100% |████████████████████████████████| 307kB 227kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.13.3 (from scikit-learn)\n",
      "  Downloading https://files.pythonhosted.org/packages/45/b2/6c7545bb7a38754d63048c7696804a0d947328125d81bf12beaa692c3ae3/numpy-1.19.5-cp36-cp36m-manylinux1_x86_64.whl (13.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 13.4MB 75kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading https://files.pythonhosted.org/packages/61/cf/6e354304bcb9c6413c4e02a747b600061c21d38ba51e7e544ac7bc66aecc/threadpoolctl-3.1.0-py3-none-any.whl\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib)\n",
      "  Downloading https://files.pythonhosted.org/packages/a7/1b/cbd8ae738719b5f41592a12057ef5442e2ed5f5cb5451f8fc7e9f8875a1a/kiwisolver-1.3.1-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.1MB 159kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting python-dateutil>=2.1 (from matplotlib)\n",
      "  Using cached https://files.pythonhosted.org/packages/36/7a/87837f39d0296e723bb9b62bbb257d0355c7f6128853c78955f57342a56d/python_dateutil-2.8.2-py2.py3-none-any.whl\n",
      "Collecting pillow>=6.2.0 (from matplotlib)\n",
      "  Downloading https://files.pythonhosted.org/packages/7d/2a/2fc11b54e2742db06297f7fa7f420a0e3069fdcf0e4b57dfec33f0b08622/Pillow-8.4.0.tar.gz (49.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 49.4MB 16kB/s ta 0:00:012\n",
      "\u001b[?25hCollecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading https://files.pythonhosted.org/packages/5c/f9/695d6bedebd747e5eb0fe8fad57b72fdf25411273a39791cde838d5a8f51/cycler-0.11.0-py3-none-any.whl\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 (from matplotlib)\n",
      "  Downloading https://files.pythonhosted.org/packages/6c/10/a7d0fa5baea8fe7b50f448ab742f26f52b80bfca85ac2be9d35cdd9a3246/pyparsing-3.0.9-py3-none-any.whl (98kB)\n",
      "\u001b[K    100% |████████████████████████████████| 102kB 19kB/s a 0:00:019\n",
      "\u001b[?25hCollecting six>=1.5 (from python-dateutil>=2.1->matplotlib)\n",
      "  Using cached https://files.pythonhosted.org/packages/d9/5a/e7c31adbe875f2abbb91bd84cf2dc52d792b5a01506781dbcf25c91daf11/six-1.16.0-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: pillow\n",
      "  Running setup.py bdist_wheel for pillow ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/arman/.cache/pip/wheels/a7/69/9a/bba9fca6782340f88dbc378893095722a663cbc618e58fe401\n",
      "Successfully built pillow\n",
      "Installing collected packages: joblib, numpy, scipy, threadpoolctl, scikit-learn, kiwisolver, six, python-dateutil, pillow, cycler, pyparsing, matplotlib\n",
      "Successfully installed cycler-0.11.0 joblib-1.1.0 kiwisolver-1.3.1 matplotlib-3.3.4 numpy-1.19.5 pillow-8.4.0 pyparsing-3.0.9 python-dateutil-2.8.2 scikit-learn-0.24.2 scipy-1.5.4 six-1.16.0 threadpoolctl-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "!pip3 install -U scikit-learn scipy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "\n",
    "def get_num_input():\n",
    "    df = pd.read_csv('Ad.csv')\n",
    "    df = df.drop(['filename'],axis=1)\n",
    "    columns = list(df.columns)\n",
    "    features = columns[:-1]\n",
    "    cls_name = columns[-1]\n",
    "    X = np.zeros((len(df), len(features)))\n",
    "    Y = list()\n",
    "    for i, row in df.iterrows():\n",
    "        X[i] = row[features]                    \n",
    "        Y.append(row[cls_name])\n",
    "\n",
    "    return (X, Y)\n",
    "X_num, Y = get_num_input()\n",
    "encoder = LabelEncoder()\n",
    "Y = encoder.fit_transform(Y)\n",
    "X_audio_train, X_audio_test, y_audio_train, y_audio_test = train_test_split( X_num, Y, test_size=0.2,random_state=42)\n",
    "#Scaling the Feature columns\n",
    "#scaler = StandardScaler()\n",
    "#X = scaler.fit_transform(np.array(data.iloc[:, :-1], dtype = float)\n",
    "\n",
    "#def get_img_input():\n",
    "    #df = pd.read_csv('/DATA/akanksha_2021cs39/visualdata.csv')\n",
    "    #X_img = np.zeros((len(df), 224, 224)) # change as per image size\n",
    "    #Y = list()\n",
    "    #for i, row in df.iterrows():\n",
    "        #X_img[i] = np.array(Image.open(row['X_img']))\n",
    "        #Y.append(row['class'])\n",
    "\n",
    "    #return (X_img, Y)\n",
    "    \n",
    "#X_img, Y_img = get_img_input() # use one of the Ys\n",
    "# X feature normalization, convert Y to one-hot representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('/DATA/akanksha_2021cs39/fakeAV_audio.csv')\n",
    "#df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(21253)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('Ad.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2 = pd.read_csv('/DATA/akanksha_2021cs39/fakeAV_visual.csv')\n",
    "#df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2 = df2.drop([0 , 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2.to_csv('Vd.csv', header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_input():\n",
    "    df = pd.read_csv('Vd.csv')\n",
    "    columns = list(df.columns)\n",
    "    features = columns[:-1]\n",
    "    cls_name = columns[-1]\n",
    "    X = np.zeros((len(df), len(features)))\n",
    "    Y = list()\n",
    "    for i, row in df.iterrows():\n",
    "        X[i] = row[features]                    \n",
    "        Y.append(row[cls_name])\n",
    "\n",
    "    return (X, Y)\n",
    "X_img, Y_img = get_img_input()\n",
    "encoder = LabelEncoder()\n",
    "Y_img = encoder.fit_transform(Y_img)\n",
    "X_img_train, X_img_test, y_img_train, y_img_test = train_test_split( X_img, Y_img, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y_audio_train # both audio and video samples are synchronized so both have same levels that is why \n",
    "                                                     #initialized here with one (audio or video) label.\n",
    "y_test=y_audio_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17001, 133)\n",
      "(17001, 50176)\n",
      "(4251, 133)\n",
      "(4251, 50176)\n",
      "(17001,)\n",
      "(17001,)\n",
      "(4251,)\n",
      "(4251,)\n"
     ]
    }
   ],
   "source": [
    "print(X_audio_train.shape)\n",
    "print(X_img_train.shape)\n",
    "print(X_audio_test.shape)\n",
    "print(X_img_test.shape)\n",
    "print(y_audio_train.shape)\n",
    "print(y_img_train.shape)\n",
    "print(y_audio_test.shape)\n",
    "print(y_img_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten, concatenate\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.layers import Dropout \n",
    "def compile_model():\n",
    "    img_input = Input(shape=(50176,)) \n",
    "    ## branch 1 with image input\n",
    "    x = Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(0.01))(img_input)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    out_a = Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    \n",
    "    #x = MaxPooling2D((2, 2))(x)\n",
    "    #x = Flatten()(x)\n",
    "    #out_a = Dense(64)(x)\n",
    "\n",
    "    num_input = Input(shape=(133,))        ## branch 2 with numerical input\n",
    "    x1 = Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(0.01))(num_input)\n",
    "    x1 = Dropout(0.3)(x1)\n",
    "    x1 = Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x1)\n",
    "    x1 = Dropout(0.3)(x1)\n",
    "    x1 = Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x1)\n",
    "    x1 = Dropout(0.3)(x1)\n",
    "    x1 = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x1)\n",
    "    x1 = Dropout(0.3)(x1)\n",
    "    x1 = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x1)\n",
    "    x1 = Dropout(0.3)(x1)\n",
    "    out_b = Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x1)\n",
    "\n",
    "    concatenated = concatenate([out_a, out_b])    ## concatenate the two branches\n",
    "    out = Dense(1, activation='sigmoid')(concatenated)\n",
    "    model = Model([img_input, num_input], out)\n",
    "    adam = Adam(lr=0.001, decay=1e-5)\n",
    "    print(model.summary())\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "    return model\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "### Just for sanity check\n",
    "\n",
    "#print(model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 50176)]      0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 133)]        0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1024)         51381248    ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 1024)         137216      ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 1024)         0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 1024)         0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 512)          524800      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 512)          524800      ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 512)          0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 512)          0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 256)          131328      ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 256)          131328      ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 256)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 256)          0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 21:12:25.601018: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/arman/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-07-05 21:12:25.601047: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-07-05 21:12:25.601069: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (arman): /proc/driver/nvidia/version does not exist\n",
      "2022-07-05 21:12:25.601439: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_3 (Dense)                (None, 128)          32896       ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 128)          32896       ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 128)          0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 128)          0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 64)           8256        ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 64)           8256        ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 64)           0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 64)           0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 32)           2080        ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 32)           2080        ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 64)           0           ['dense_5[0][0]',                \n",
      "                                                                  'dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 1)            65          ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 52,917,249\n",
      "Trainable params: 52,917,249\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = compile_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "67/67 [==============================] - 15s 205ms/step - loss: 203.9039 - accuracy: 0.8860 - val_loss: 53.9685 - val_accuracy: 0.9475\n",
      "Epoch 2/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 55.3476 - accuracy: 0.8914 - val_loss: 38.0840 - val_accuracy: 0.3331\n",
      "Epoch 3/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 39.2560 - accuracy: 0.8918 - val_loss: 31.9311 - val_accuracy: 0.8224\n",
      "Epoch 4/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 31.1999 - accuracy: 0.8844 - val_loss: 27.5281 - val_accuracy: 0.9475\n",
      "Epoch 5/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 26.4094 - accuracy: 0.8973 - val_loss: 24.2132 - val_accuracy: 0.9475\n",
      "Epoch 6/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 23.1033 - accuracy: 0.9318 - val_loss: 21.5594 - val_accuracy: 0.9475\n",
      "Epoch 7/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 20.5697 - accuracy: 0.9401 - val_loss: 19.3624 - val_accuracy: 0.9475\n",
      "Epoch 8/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 18.5548 - accuracy: 0.9413 - val_loss: 17.5907 - val_accuracy: 0.9475\n",
      "Epoch 9/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 16.9015 - accuracy: 0.9417 - val_loss: 16.0545 - val_accuracy: 0.9475\n",
      "Epoch 10/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 15.4723 - accuracy: 0.9435 - val_loss: 14.8001 - val_accuracy: 0.9475\n",
      "Epoch 11/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 14.3521 - accuracy: 0.9437 - val_loss: 13.7933 - val_accuracy: 0.9475\n",
      "Epoch 12/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 13.4113 - accuracy: 0.9418 - val_loss: 12.9427 - val_accuracy: 0.9475\n",
      "Epoch 13/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 12.5297 - accuracy: 0.9444 - val_loss: 12.1147 - val_accuracy: 0.9475\n",
      "Epoch 14/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 11.8269 - accuracy: 0.9434 - val_loss: 11.4783 - val_accuracy: 0.9475\n",
      "Epoch 15/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 11.2074 - accuracy: 0.9452 - val_loss: 10.8797 - val_accuracy: 0.9475\n",
      "Epoch 16/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 10.6545 - accuracy: 0.9453 - val_loss: 10.3789 - val_accuracy: 0.9475\n",
      "Epoch 17/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 10.2039 - accuracy: 0.9443 - val_loss: 9.9566 - val_accuracy: 0.9475\n",
      "Epoch 18/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 9.7714 - accuracy: 0.9454 - val_loss: 9.5616 - val_accuracy: 0.9475\n",
      "Epoch 19/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 9.3954 - accuracy: 0.9457 - val_loss: 9.1956 - val_accuracy: 0.9475\n",
      "Epoch 20/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 9.1316 - accuracy: 0.9441 - val_loss: 8.9486 - val_accuracy: 0.9475\n",
      "Epoch 21/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 8.8604 - accuracy: 0.9443 - val_loss: 8.6541 - val_accuracy: 0.9475\n",
      "Epoch 22/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 8.5452 - accuracy: 0.9450 - val_loss: 8.3713 - val_accuracy: 0.9475\n",
      "Epoch 23/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 8.2686 - accuracy: 0.9457 - val_loss: 8.1244 - val_accuracy: 0.9475\n",
      "Epoch 24/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 8.1062 - accuracy: 0.9432 - val_loss: 7.9383 - val_accuracy: 0.9475\n",
      "Epoch 25/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 7.8589 - accuracy: 0.9451 - val_loss: 7.7089 - val_accuracy: 0.9475\n",
      "Epoch 26/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 7.6554 - accuracy: 0.9444 - val_loss: 7.6614 - val_accuracy: 0.9475\n",
      "Epoch 27/1000\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 7.4935 - accuracy: 0.9453 - val_loss: 7.3238 - val_accuracy: 0.9475\n",
      "Epoch 28/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 7.2777 - accuracy: 0.9450 - val_loss: 7.1466 - val_accuracy: 0.9475\n",
      "Epoch 29/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 7.1357 - accuracy: 0.9442 - val_loss: 7.0211 - val_accuracy: 0.9475\n",
      "Epoch 30/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 7.0046 - accuracy: 0.9434 - val_loss: 6.8811 - val_accuracy: 0.9475\n",
      "Epoch 31/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 6.7892 - accuracy: 0.9455 - val_loss: 6.7138 - val_accuracy: 0.9475\n",
      "Epoch 32/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 6.6128 - accuracy: 0.9459 - val_loss: 6.5188 - val_accuracy: 0.9475\n",
      "Epoch 33/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 6.4702 - accuracy: 0.9456 - val_loss: 6.4000 - val_accuracy: 0.9475\n",
      "Epoch 34/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 6.3684 - accuracy: 0.9437 - val_loss: 6.3424 - val_accuracy: 0.9475\n",
      "Epoch 35/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 6.2822 - accuracy: 0.9431 - val_loss: 6.1667 - val_accuracy: 0.9475\n",
      "Epoch 36/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 6.1398 - accuracy: 0.9449 - val_loss: 6.0142 - val_accuracy: 0.9475\n",
      "Epoch 37/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 5.9465 - accuracy: 0.9458 - val_loss: 5.8673 - val_accuracy: 0.9475\n",
      "Epoch 38/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 5.9043 - accuracy: 0.9441 - val_loss: 5.8284 - val_accuracy: 0.9475\n",
      "Epoch 39/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 5.7571 - accuracy: 0.9454 - val_loss: 5.6533 - val_accuracy: 0.9475\n",
      "Epoch 40/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 5.6767 - accuracy: 0.9439 - val_loss: 5.5745 - val_accuracy: 0.9475\n",
      "Epoch 41/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 5.5134 - accuracy: 0.9454 - val_loss: 5.4151 - val_accuracy: 0.9475\n",
      "Epoch 42/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 5.3911 - accuracy: 0.9452 - val_loss: 5.3214 - val_accuracy: 0.9475\n",
      "Epoch 43/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 5.2738 - accuracy: 0.9457 - val_loss: 5.2055 - val_accuracy: 0.9475\n",
      "Epoch 44/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 5.2168 - accuracy: 0.9444 - val_loss: 5.1211 - val_accuracy: 0.9475\n",
      "Epoch 45/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 5.0658 - accuracy: 0.9455 - val_loss: 4.9904 - val_accuracy: 0.9475\n",
      "Epoch 46/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 4.9676 - accuracy: 0.9448 - val_loss: 4.9361 - val_accuracy: 0.9475\n",
      "Epoch 47/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 4.8873 - accuracy: 0.9457 - val_loss: 4.8147 - val_accuracy: 0.9475\n",
      "Epoch 48/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 4.7755 - accuracy: 0.9455 - val_loss: 4.7075 - val_accuracy: 0.9475\n",
      "Epoch 49/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 4.7142 - accuracy: 0.9447 - val_loss: 4.6194 - val_accuracy: 0.9475\n",
      "Epoch 50/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 4.6077 - accuracy: 0.9454 - val_loss: 4.5924 - val_accuracy: 0.9475\n",
      "Epoch 51/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 4.5421 - accuracy: 0.9444 - val_loss: 4.4710 - val_accuracy: 0.9475\n",
      "Epoch 52/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 4.4401 - accuracy: 0.9448 - val_loss: 4.3815 - val_accuracy: 0.9475\n",
      "Epoch 53/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 4.3372 - accuracy: 0.9450 - val_loss: 4.3174 - val_accuracy: 0.9475\n",
      "Epoch 54/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 4.2421 - accuracy: 0.9457 - val_loss: 4.1701 - val_accuracy: 0.9475\n",
      "Epoch 55/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 4.1385 - accuracy: 0.9455 - val_loss: 4.1167 - val_accuracy: 0.9475\n",
      "Epoch 56/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 4.0540 - accuracy: 0.9455 - val_loss: 3.9716 - val_accuracy: 0.9475\n",
      "Epoch 57/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 3.9890 - accuracy: 0.9443 - val_loss: 3.9431 - val_accuracy: 0.9475\n",
      "Epoch 58/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 3.9059 - accuracy: 0.9452 - val_loss: 3.8315 - val_accuracy: 0.9475\n",
      "Epoch 59/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 3.8398 - accuracy: 0.9441 - val_loss: 3.7591 - val_accuracy: 0.9475\n",
      "Epoch 60/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 3.7290 - accuracy: 0.9452 - val_loss: 3.6588 - val_accuracy: 0.9475\n",
      "Epoch 61/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 3.6610 - accuracy: 0.9455 - val_loss: 3.6192 - val_accuracy: 0.9475\n",
      "Epoch 62/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 3.6345 - accuracy: 0.9430 - val_loss: 3.6991 - val_accuracy: 0.9475\n",
      "Epoch 63/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 3.5778 - accuracy: 0.9438 - val_loss: 3.5144 - val_accuracy: 0.9475\n",
      "Epoch 64/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 3.4695 - accuracy: 0.9445 - val_loss: 3.3601 - val_accuracy: 0.9475\n",
      "Epoch 65/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 3.3592 - accuracy: 0.9451 - val_loss: 3.3109 - val_accuracy: 0.9475\n",
      "Epoch 66/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 3.3034 - accuracy: 0.9454 - val_loss: 3.2253 - val_accuracy: 0.9475\n",
      "Epoch 67/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 3.1782 - accuracy: 0.9456 - val_loss: 3.1354 - val_accuracy: 0.9475\n",
      "Epoch 68/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 3.1167 - accuracy: 0.9455 - val_loss: 3.0766 - val_accuracy: 0.9475\n",
      "Epoch 69/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 3.0525 - accuracy: 0.9452 - val_loss: 3.0347 - val_accuracy: 0.9475\n",
      "Epoch 70/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 3.0112 - accuracy: 0.9448 - val_loss: 2.9388 - val_accuracy: 0.9475\n",
      "Epoch 71/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 2.9576 - accuracy: 0.9442 - val_loss: 2.8569 - val_accuracy: 0.9475\n",
      "Epoch 72/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 2.8527 - accuracy: 0.9453 - val_loss: 2.8351 - val_accuracy: 0.9475\n",
      "Epoch 73/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 2.7718 - accuracy: 0.9457 - val_loss: 2.6970 - val_accuracy: 0.9475\n",
      "Epoch 74/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 2.6958 - accuracy: 0.9455 - val_loss: 2.6295 - val_accuracy: 0.9475\n",
      "Epoch 75/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 2.6094 - accuracy: 0.9456 - val_loss: 2.6090 - val_accuracy: 0.9475\n",
      "Epoch 76/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 2.5862 - accuracy: 0.9455 - val_loss: 2.5317 - val_accuracy: 0.9475\n",
      "Epoch 77/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 2.5244 - accuracy: 0.9447 - val_loss: 2.4838 - val_accuracy: 0.9475\n",
      "Epoch 78/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 2.4683 - accuracy: 0.9450 - val_loss: 2.3910 - val_accuracy: 0.9475\n",
      "Epoch 79/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 2.4304 - accuracy: 0.9439 - val_loss: 2.3531 - val_accuracy: 0.9475\n",
      "Epoch 80/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 2.3138 - accuracy: 0.9457 - val_loss: 2.2505 - val_accuracy: 0.9475\n",
      "Epoch 81/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 2.2852 - accuracy: 0.9451 - val_loss: 2.2139 - val_accuracy: 0.9475\n",
      "Epoch 82/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 2.2035 - accuracy: 0.9455 - val_loss: 2.1337 - val_accuracy: 0.9475\n",
      "Epoch 83/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 2.1021 - accuracy: 0.9459 - val_loss: 2.0598 - val_accuracy: 0.9475\n",
      "Epoch 84/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 2.0337 - accuracy: 0.9459 - val_loss: 1.9843 - val_accuracy: 0.9475\n",
      "Epoch 85/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 1.9849 - accuracy: 0.9455 - val_loss: 1.9511 - val_accuracy: 0.9475\n",
      "Epoch 86/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 1.9063 - accuracy: 0.9458 - val_loss: 1.8630 - val_accuracy: 0.9475\n",
      "Epoch 87/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 1.8418 - accuracy: 0.9459 - val_loss: 1.8095 - val_accuracy: 0.9475\n",
      "Epoch 88/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 1.8029 - accuracy: 0.9458 - val_loss: 1.7690 - val_accuracy: 0.9475\n",
      "Epoch 89/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 1.7584 - accuracy: 0.9458 - val_loss: 1.7091 - val_accuracy: 0.9475\n",
      "Epoch 90/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 1.6800 - accuracy: 0.9459 - val_loss: 1.6386 - val_accuracy: 0.9475\n",
      "Epoch 91/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 1.6260 - accuracy: 0.9459 - val_loss: 1.5949 - val_accuracy: 0.9475\n",
      "Epoch 92/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 1.5800 - accuracy: 0.9458 - val_loss: 1.5466 - val_accuracy: 0.9475\n",
      "Epoch 93/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 1.5434 - accuracy: 0.9458 - val_loss: 1.5071 - val_accuracy: 0.9475\n",
      "Epoch 94/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 1.4752 - accuracy: 0.9458 - val_loss: 1.4343 - val_accuracy: 0.9475\n",
      "Epoch 95/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 1.4245 - accuracy: 0.9459 - val_loss: 1.3896 - val_accuracy: 0.9475\n",
      "Epoch 96/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 1.3749 - accuracy: 0.9459 - val_loss: 1.3376 - val_accuracy: 0.9475\n",
      "Epoch 97/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 1.3387 - accuracy: 0.9457 - val_loss: 1.3068 - val_accuracy: 0.9475\n",
      "Epoch 98/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 1.2842 - accuracy: 0.9459 - val_loss: 1.2643 - val_accuracy: 0.9475\n",
      "Epoch 99/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 1.2525 - accuracy: 0.9459 - val_loss: 1.2221 - val_accuracy: 0.9475\n",
      "Epoch 100/1000\n",
      "67/67 [==============================] - 13s 197ms/step - loss: 1.2068 - accuracy: 0.9459 - val_loss: 1.1822 - val_accuracy: 0.9475\n",
      "Epoch 101/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 1.1695 - accuracy: 0.9459 - val_loss: 1.1370 - val_accuracy: 0.9475\n",
      "Epoch 102/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 1.1289 - accuracy: 0.9459 - val_loss: 1.0972 - val_accuracy: 0.9475\n",
      "Epoch 103/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 1.0802 - accuracy: 0.9459 - val_loss: 1.0486 - val_accuracy: 0.9475\n",
      "Epoch 104/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 1.0456 - accuracy: 0.9459 - val_loss: 1.0256 - val_accuracy: 0.9475\n",
      "Epoch 105/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 1.0111 - accuracy: 0.9459 - val_loss: 0.9789 - val_accuracy: 0.9475\n",
      "Epoch 106/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.9732 - accuracy: 0.9459 - val_loss: 0.9511 - val_accuracy: 0.9475\n",
      "Epoch 107/1000\n",
      "67/67 [==============================] - 13s 196ms/step - loss: 0.9441 - accuracy: 0.9458 - val_loss: 0.9128 - val_accuracy: 0.9475\n",
      "Epoch 108/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.9070 - accuracy: 0.9459 - val_loss: 0.8779 - val_accuracy: 0.9475\n",
      "Epoch 109/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.8809 - accuracy: 0.9459 - val_loss: 0.8512 - val_accuracy: 0.9475\n",
      "Epoch 110/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.8430 - accuracy: 0.9459 - val_loss: 0.8164 - val_accuracy: 0.9475\n",
      "Epoch 111/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.8113 - accuracy: 0.9459 - val_loss: 0.7867 - val_accuracy: 0.9475\n",
      "Epoch 112/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.7788 - accuracy: 0.9459 - val_loss: 0.7568 - val_accuracy: 0.9475\n",
      "Epoch 113/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.7454 - accuracy: 0.9459 - val_loss: 0.7240 - val_accuracy: 0.9475\n",
      "Epoch 114/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.7203 - accuracy: 0.9459 - val_loss: 0.6997 - val_accuracy: 0.9475\n",
      "Epoch 115/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.6955 - accuracy: 0.9459 - val_loss: 0.6810 - val_accuracy: 0.9475\n",
      "Epoch 116/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.6735 - accuracy: 0.9459 - val_loss: 0.6548 - val_accuracy: 0.9475\n",
      "Epoch 117/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.6437 - accuracy: 0.9459 - val_loss: 0.6248 - val_accuracy: 0.9475\n",
      "Epoch 118/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.6174 - accuracy: 0.9459 - val_loss: 0.5969 - val_accuracy: 0.9475\n",
      "Epoch 119/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.5951 - accuracy: 0.9459 - val_loss: 0.5762 - val_accuracy: 0.9475\n",
      "Epoch 120/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.5736 - accuracy: 0.9459 - val_loss: 0.5543 - val_accuracy: 0.9475\n",
      "Epoch 121/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.5540 - accuracy: 0.9459 - val_loss: 0.5368 - val_accuracy: 0.9475\n",
      "Epoch 122/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.5344 - accuracy: 0.9459 - val_loss: 0.5190 - val_accuracy: 0.9475\n",
      "Epoch 123/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.5144 - accuracy: 0.9459 - val_loss: 0.4987 - val_accuracy: 0.9475\n",
      "Epoch 124/1000\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 0.4991 - accuracy: 0.9459 - val_loss: 0.4815 - val_accuracy: 0.9475\n",
      "Epoch 125/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.4788 - accuracy: 0.9459 - val_loss: 0.4622 - val_accuracy: 0.9475\n",
      "Epoch 126/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.4628 - accuracy: 0.9459 - val_loss: 0.4462 - val_accuracy: 0.9475\n",
      "Epoch 127/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.4464 - accuracy: 0.9459 - val_loss: 0.4310 - val_accuracy: 0.9475\n",
      "Epoch 128/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.4358 - accuracy: 0.9459 - val_loss: 0.4216 - val_accuracy: 0.9475\n",
      "Epoch 129/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.4200 - accuracy: 0.9459 - val_loss: 0.4061 - val_accuracy: 0.9475\n",
      "Epoch 130/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.4066 - accuracy: 0.9459 - val_loss: 0.3928 - val_accuracy: 0.9475\n",
      "Epoch 131/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.3940 - accuracy: 0.9459 - val_loss: 0.3807 - val_accuracy: 0.9475\n",
      "Epoch 132/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.3804 - accuracy: 0.9459 - val_loss: 0.3774 - val_accuracy: 0.9475\n",
      "Epoch 133/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.3706 - accuracy: 0.9459 - val_loss: 0.3567 - val_accuracy: 0.9475\n",
      "Epoch 134/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.3592 - accuracy: 0.9459 - val_loss: 0.3458 - val_accuracy: 0.9475\n",
      "Epoch 135/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.3484 - accuracy: 0.9459 - val_loss: 0.3399 - val_accuracy: 0.9475\n",
      "Epoch 136/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.3401 - accuracy: 0.9459 - val_loss: 0.3264 - val_accuracy: 0.9475\n",
      "Epoch 137/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.3282 - accuracy: 0.9459 - val_loss: 0.3177 - val_accuracy: 0.9475\n",
      "Epoch 138/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.3204 - accuracy: 0.9459 - val_loss: 0.3085 - val_accuracy: 0.9475\n",
      "Epoch 139/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.3124 - accuracy: 0.9459 - val_loss: 0.3013 - val_accuracy: 0.9475\n",
      "Epoch 140/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.3060 - accuracy: 0.9459 - val_loss: 0.2961 - val_accuracy: 0.9475\n",
      "Epoch 141/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2974 - accuracy: 0.9459 - val_loss: 0.2898 - val_accuracy: 0.9475\n",
      "Epoch 142/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2947 - accuracy: 0.9459 - val_loss: 0.2921 - val_accuracy: 0.9475\n",
      "Epoch 143/1000\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 0.2858 - accuracy: 0.9459 - val_loss: 0.2771 - val_accuracy: 0.9475\n",
      "Epoch 144/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2793 - accuracy: 0.9459 - val_loss: 0.2691 - val_accuracy: 0.9475\n",
      "Epoch 145/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2739 - accuracy: 0.9459 - val_loss: 0.2672 - val_accuracy: 0.9475\n",
      "Epoch 146/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2701 - accuracy: 0.9459 - val_loss: 0.2666 - val_accuracy: 0.9475\n",
      "Epoch 147/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2653 - accuracy: 0.9459 - val_loss: 0.2546 - val_accuracy: 0.9475\n",
      "Epoch 148/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2587 - accuracy: 0.9459 - val_loss: 0.2585 - val_accuracy: 0.9475\n",
      "Epoch 149/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2567 - accuracy: 0.9459 - val_loss: 0.2467 - val_accuracy: 0.9475\n",
      "Epoch 150/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2505 - accuracy: 0.9459 - val_loss: 0.2434 - val_accuracy: 0.9475\n",
      "Epoch 151/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2480 - accuracy: 0.9459 - val_loss: 0.2394 - val_accuracy: 0.9475\n",
      "Epoch 152/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2450 - accuracy: 0.9459 - val_loss: 0.2371 - val_accuracy: 0.9475\n",
      "Epoch 153/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2402 - accuracy: 0.9459 - val_loss: 0.2327 - val_accuracy: 0.9475\n",
      "Epoch 154/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2392 - accuracy: 0.9459 - val_loss: 0.2313 - val_accuracy: 0.9475\n",
      "Epoch 155/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2353 - accuracy: 0.9459 - val_loss: 0.2285 - val_accuracy: 0.9475\n",
      "Epoch 156/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2329 - accuracy: 0.9459 - val_loss: 0.2300 - val_accuracy: 0.9475\n",
      "Epoch 157/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2320 - accuracy: 0.9459 - val_loss: 0.2254 - val_accuracy: 0.9475\n",
      "Epoch 158/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2288 - accuracy: 0.9459 - val_loss: 0.2261 - val_accuracy: 0.9475\n",
      "Epoch 159/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2263 - accuracy: 0.9459 - val_loss: 0.2219 - val_accuracy: 0.9475\n",
      "Epoch 160/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2249 - accuracy: 0.9459 - val_loss: 0.2182 - val_accuracy: 0.9475\n",
      "Epoch 161/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2233 - accuracy: 0.9459 - val_loss: 0.2162 - val_accuracy: 0.9475\n",
      "Epoch 162/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2222 - accuracy: 0.9459 - val_loss: 0.2145 - val_accuracy: 0.9475\n",
      "Epoch 163/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2207 - accuracy: 0.9459 - val_loss: 0.2182 - val_accuracy: 0.9475\n",
      "Epoch 164/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2199 - accuracy: 0.9459 - val_loss: 0.2130 - val_accuracy: 0.9475\n",
      "Epoch 165/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2179 - accuracy: 0.9459 - val_loss: 0.2106 - val_accuracy: 0.9475\n",
      "Epoch 166/1000\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 0.2181 - accuracy: 0.9459 - val_loss: 0.2113 - val_accuracy: 0.9475\n",
      "Epoch 167/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2163 - accuracy: 0.9459 - val_loss: 0.2089 - val_accuracy: 0.9475\n",
      "Epoch 168/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2157 - accuracy: 0.9459 - val_loss: 0.2112 - val_accuracy: 0.9475\n",
      "Epoch 169/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2165 - accuracy: 0.9459 - val_loss: 0.2072 - val_accuracy: 0.9475\n",
      "Epoch 170/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2149 - accuracy: 0.9459 - val_loss: 0.2081 - val_accuracy: 0.9475\n",
      "Epoch 171/1000\n",
      "67/67 [==============================] - 13s 196ms/step - loss: 0.2140 - accuracy: 0.9459 - val_loss: 0.2080 - val_accuracy: 0.9475\n",
      "Epoch 172/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2132 - accuracy: 0.9459 - val_loss: 0.2052 - val_accuracy: 0.9475\n",
      "Epoch 173/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2110 - accuracy: 0.9459 - val_loss: 0.2059 - val_accuracy: 0.9475\n",
      "Epoch 174/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2111 - accuracy: 0.9459 - val_loss: 0.2142 - val_accuracy: 0.9475\n",
      "Epoch 175/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2119 - accuracy: 0.9459 - val_loss: 0.2047 - val_accuracy: 0.9475\n",
      "Epoch 176/1000\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 0.2101 - accuracy: 0.9459 - val_loss: 0.2031 - val_accuracy: 0.9475\n",
      "Epoch 177/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2103 - accuracy: 0.9459 - val_loss: 0.2030 - val_accuracy: 0.9475\n",
      "Epoch 178/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2102 - accuracy: 0.9459 - val_loss: 0.2026 - val_accuracy: 0.9475\n",
      "Epoch 179/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2094 - accuracy: 0.9459 - val_loss: 0.2025 - val_accuracy: 0.9475\n",
      "Epoch 180/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2108 - accuracy: 0.9459 - val_loss: 0.2031 - val_accuracy: 0.9475\n",
      "Epoch 181/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2087 - accuracy: 0.9459 - val_loss: 0.2047 - val_accuracy: 0.9475\n",
      "Epoch 182/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2083 - accuracy: 0.9459 - val_loss: 0.2045 - val_accuracy: 0.9475\n",
      "Epoch 183/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2090 - accuracy: 0.9459 - val_loss: 0.2020 - val_accuracy: 0.9475\n",
      "Epoch 184/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2081 - accuracy: 0.9459 - val_loss: 0.2053 - val_accuracy: 0.9475\n",
      "Epoch 185/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2092 - accuracy: 0.9459 - val_loss: 0.2038 - val_accuracy: 0.9475\n",
      "Epoch 186/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2094 - accuracy: 0.9459 - val_loss: 0.2012 - val_accuracy: 0.9475\n",
      "Epoch 187/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2075 - accuracy: 0.9459 - val_loss: 0.2003 - val_accuracy: 0.9475\n",
      "Epoch 188/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2078 - accuracy: 0.9459 - val_loss: 0.2007 - val_accuracy: 0.9475\n",
      "Epoch 189/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2084 - accuracy: 0.9459 - val_loss: 0.2020 - val_accuracy: 0.9475\n",
      "Epoch 190/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2072 - accuracy: 0.9459 - val_loss: 0.2013 - val_accuracy: 0.9475\n",
      "Epoch 191/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2081 - accuracy: 0.9459 - val_loss: 0.2016 - val_accuracy: 0.9475\n",
      "Epoch 192/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2068 - accuracy: 0.9459 - val_loss: 0.2008 - val_accuracy: 0.9475\n",
      "Epoch 193/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2069 - accuracy: 0.9459 - val_loss: 0.2006 - val_accuracy: 0.9475\n",
      "Epoch 194/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2094 - accuracy: 0.9459 - val_loss: 0.2044 - val_accuracy: 0.9475\n",
      "Epoch 195/1000\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 0.2071 - accuracy: 0.9459 - val_loss: 0.2027 - val_accuracy: 0.9475\n",
      "Epoch 196/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2067 - accuracy: 0.9459 - val_loss: 0.2014 - val_accuracy: 0.9475\n",
      "Epoch 197/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2063 - accuracy: 0.9459 - val_loss: 0.2005 - val_accuracy: 0.9475\n",
      "Epoch 198/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2076 - accuracy: 0.9459 - val_loss: 0.2027 - val_accuracy: 0.9475\n",
      "Epoch 199/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2077 - accuracy: 0.9459 - val_loss: 0.1999 - val_accuracy: 0.9475\n",
      "Epoch 200/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2064 - accuracy: 0.9459 - val_loss: 0.2010 - val_accuracy: 0.9475\n",
      "Epoch 201/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2082 - accuracy: 0.9459 - val_loss: 0.2003 - val_accuracy: 0.9475\n",
      "Epoch 202/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2074 - accuracy: 0.9459 - val_loss: 0.2008 - val_accuracy: 0.9475\n",
      "Epoch 203/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2070 - accuracy: 0.9459 - val_loss: 0.2006 - val_accuracy: 0.9475\n",
      "Epoch 204/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2074 - accuracy: 0.9459 - val_loss: 0.1999 - val_accuracy: 0.9475\n",
      "Epoch 205/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2060 - accuracy: 0.9459 - val_loss: 0.2005 - val_accuracy: 0.9475\n",
      "Epoch 206/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2067 - accuracy: 0.9459 - val_loss: 0.1997 - val_accuracy: 0.9475\n",
      "Epoch 207/1000\n",
      "67/67 [==============================] - 13s 196ms/step - loss: 0.2056 - accuracy: 0.9459 - val_loss: 0.2000 - val_accuracy: 0.9475\n",
      "Epoch 208/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2061 - accuracy: 0.9459 - val_loss: 0.1993 - val_accuracy: 0.9475\n",
      "Epoch 209/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2058 - accuracy: 0.9459 - val_loss: 0.1990 - val_accuracy: 0.9475\n",
      "Epoch 210/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2080 - accuracy: 0.9459 - val_loss: 0.2005 - val_accuracy: 0.9475\n",
      "Epoch 211/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2086 - accuracy: 0.9459 - val_loss: 0.1995 - val_accuracy: 0.9475\n",
      "Epoch 212/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2057 - accuracy: 0.9459 - val_loss: 0.1993 - val_accuracy: 0.9475\n",
      "Epoch 213/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2066 - accuracy: 0.9459 - val_loss: 0.2003 - val_accuracy: 0.9475\n",
      "Epoch 214/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2074 - accuracy: 0.9459 - val_loss: 0.2001 - val_accuracy: 0.9475\n",
      "Epoch 215/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2073 - accuracy: 0.9459 - val_loss: 0.1998 - val_accuracy: 0.9475\n",
      "Epoch 216/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2061 - accuracy: 0.9459 - val_loss: 0.2029 - val_accuracy: 0.9475\n",
      "Epoch 217/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2060 - accuracy: 0.9459 - val_loss: 0.2017 - val_accuracy: 0.9475\n",
      "Epoch 218/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2089 - accuracy: 0.9459 - val_loss: 0.2042 - val_accuracy: 0.9475\n",
      "Epoch 219/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2077 - accuracy: 0.9459 - val_loss: 0.2002 - val_accuracy: 0.9475\n",
      "Epoch 220/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2068 - accuracy: 0.9459 - val_loss: 0.1997 - val_accuracy: 0.9475\n",
      "Epoch 221/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2050 - accuracy: 0.9459 - val_loss: 0.2007 - val_accuracy: 0.9475\n",
      "Epoch 222/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2067 - accuracy: 0.9459 - val_loss: 0.1997 - val_accuracy: 0.9475\n",
      "Epoch 223/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2061 - accuracy: 0.9459 - val_loss: 0.1993 - val_accuracy: 0.9475\n",
      "Epoch 224/1000\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 0.2063 - accuracy: 0.9459 - val_loss: 0.2013 - val_accuracy: 0.9475\n",
      "Epoch 225/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2061 - accuracy: 0.9459 - val_loss: 0.2016 - val_accuracy: 0.9475\n",
      "Epoch 226/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2056 - accuracy: 0.9459 - val_loss: 0.2013 - val_accuracy: 0.9475\n",
      "Epoch 227/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2063 - accuracy: 0.9459 - val_loss: 0.1995 - val_accuracy: 0.9475\n",
      "Epoch 228/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2066 - accuracy: 0.9459 - val_loss: 0.1994 - val_accuracy: 0.9475\n",
      "Epoch 229/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2059 - accuracy: 0.9459 - val_loss: 0.2012 - val_accuracy: 0.9475\n",
      "Epoch 230/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2051 - accuracy: 0.9459 - val_loss: 0.2005 - val_accuracy: 0.9475\n",
      "Epoch 231/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2071 - accuracy: 0.9459 - val_loss: 0.2033 - val_accuracy: 0.9475\n",
      "Epoch 232/1000\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 0.2066 - accuracy: 0.9459 - val_loss: 0.2052 - val_accuracy: 0.9475\n",
      "Epoch 233/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2077 - accuracy: 0.9459 - val_loss: 0.1997 - val_accuracy: 0.9475\n",
      "Epoch 234/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2047 - accuracy: 0.9459 - val_loss: 0.2006 - val_accuracy: 0.9475\n",
      "Epoch 235/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2060 - accuracy: 0.9459 - val_loss: 0.1990 - val_accuracy: 0.9475\n",
      "Epoch 236/1000\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 0.2065 - accuracy: 0.9459 - val_loss: 0.2013 - val_accuracy: 0.9475\n",
      "Epoch 237/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2063 - accuracy: 0.9459 - val_loss: 0.1994 - val_accuracy: 0.9475\n",
      "Epoch 238/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2066 - accuracy: 0.9459 - val_loss: 0.1991 - val_accuracy: 0.9475\n",
      "Epoch 239/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2055 - accuracy: 0.9459 - val_loss: 0.2003 - val_accuracy: 0.9475\n",
      "Epoch 240/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2060 - accuracy: 0.9459 - val_loss: 0.1993 - val_accuracy: 0.9475\n",
      "Epoch 241/1000\n",
      "67/67 [==============================] - 13s 197ms/step - loss: 0.2079 - accuracy: 0.9459 - val_loss: 0.2045 - val_accuracy: 0.9475\n",
      "Epoch 242/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2073 - accuracy: 0.9459 - val_loss: 0.1996 - val_accuracy: 0.9475\n",
      "Epoch 243/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2054 - accuracy: 0.9459 - val_loss: 0.1992 - val_accuracy: 0.9475\n",
      "Epoch 244/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2072 - accuracy: 0.9459 - val_loss: 0.2026 - val_accuracy: 0.9475\n",
      "Epoch 245/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2057 - accuracy: 0.9459 - val_loss: 0.1986 - val_accuracy: 0.9475\n",
      "Epoch 246/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2066 - accuracy: 0.9459 - val_loss: 0.2025 - val_accuracy: 0.9475\n",
      "Epoch 247/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2052 - accuracy: 0.9459 - val_loss: 0.1985 - val_accuracy: 0.9475\n",
      "Epoch 248/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2066 - accuracy: 0.9459 - val_loss: 0.2002 - val_accuracy: 0.9475\n",
      "Epoch 249/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2068 - accuracy: 0.9459 - val_loss: 0.1990 - val_accuracy: 0.9475\n",
      "Epoch 250/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2053 - accuracy: 0.9459 - val_loss: 0.1995 - val_accuracy: 0.9475\n",
      "Epoch 251/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2047 - accuracy: 0.9459 - val_loss: 0.1985 - val_accuracy: 0.9475\n",
      "Epoch 252/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2049 - accuracy: 0.9459 - val_loss: 0.1983 - val_accuracy: 0.9475\n",
      "Epoch 253/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2044 - accuracy: 0.9459 - val_loss: 0.1984 - val_accuracy: 0.9475\n",
      "Epoch 254/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2056 - accuracy: 0.9459 - val_loss: 0.1997 - val_accuracy: 0.9475\n",
      "Epoch 255/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2031 - accuracy: 0.9459 - val_loss: 0.1981 - val_accuracy: 0.9475\n",
      "Epoch 256/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2053 - accuracy: 0.9459 - val_loss: 0.1991 - val_accuracy: 0.9475\n",
      "Epoch 257/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2084 - accuracy: 0.9459 - val_loss: 0.2003 - val_accuracy: 0.9475\n",
      "Epoch 258/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2052 - accuracy: 0.9459 - val_loss: 0.1984 - val_accuracy: 0.9475\n",
      "Epoch 259/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2059 - accuracy: 0.9459 - val_loss: 0.1981 - val_accuracy: 0.9475\n",
      "Epoch 260/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2043 - accuracy: 0.9459 - val_loss: 0.1994 - val_accuracy: 0.9475\n",
      "Epoch 261/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2047 - accuracy: 0.9459 - val_loss: 0.2014 - val_accuracy: 0.9475\n",
      "Epoch 262/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2037 - accuracy: 0.9459 - val_loss: 0.1983 - val_accuracy: 0.9475\n",
      "Epoch 263/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2046 - accuracy: 0.9459 - val_loss: 0.1993 - val_accuracy: 0.9475\n",
      "Epoch 264/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2050 - accuracy: 0.9459 - val_loss: 0.1987 - val_accuracy: 0.9475\n",
      "Epoch 265/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2048 - accuracy: 0.9459 - val_loss: 0.2004 - val_accuracy: 0.9475\n",
      "Epoch 266/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2065 - accuracy: 0.9459 - val_loss: 0.1997 - val_accuracy: 0.9475\n",
      "Epoch 267/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2041 - accuracy: 0.9459 - val_loss: 0.2005 - val_accuracy: 0.9475\n",
      "Epoch 268/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2056 - accuracy: 0.9459 - val_loss: 0.1993 - val_accuracy: 0.9475\n",
      "Epoch 269/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2062 - accuracy: 0.9459 - val_loss: 0.1984 - val_accuracy: 0.9475\n",
      "Epoch 270/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2042 - accuracy: 0.9459 - val_loss: 0.1981 - val_accuracy: 0.9475\n",
      "Epoch 271/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2051 - accuracy: 0.9459 - val_loss: 0.2041 - val_accuracy: 0.9475\n",
      "Epoch 272/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2056 - accuracy: 0.9459 - val_loss: 0.1992 - val_accuracy: 0.9475\n",
      "Epoch 273/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2048 - accuracy: 0.9459 - val_loss: 0.1994 - val_accuracy: 0.9475\n",
      "Epoch 274/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2056 - accuracy: 0.9459 - val_loss: 0.1989 - val_accuracy: 0.9475\n",
      "Epoch 275/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2053 - accuracy: 0.9459 - val_loss: 0.2001 - val_accuracy: 0.9475\n",
      "Epoch 276/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2046 - accuracy: 0.9459 - val_loss: 0.2033 - val_accuracy: 0.9475\n",
      "Epoch 277/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2051 - accuracy: 0.9459 - val_loss: 0.2004 - val_accuracy: 0.9475\n",
      "Epoch 278/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2060 - accuracy: 0.9459 - val_loss: 0.1989 - val_accuracy: 0.9475\n",
      "Epoch 279/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2049 - accuracy: 0.9459 - val_loss: 0.1982 - val_accuracy: 0.9475\n",
      "Epoch 280/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2057 - accuracy: 0.9459 - val_loss: 0.1983 - val_accuracy: 0.9475\n",
      "Epoch 281/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2048 - accuracy: 0.9459 - val_loss: 0.2003 - val_accuracy: 0.9475\n",
      "Epoch 282/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2067 - accuracy: 0.9459 - val_loss: 0.1985 - val_accuracy: 0.9475\n",
      "Epoch 283/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2050 - accuracy: 0.9459 - val_loss: 0.1980 - val_accuracy: 0.9475\n",
      "Epoch 284/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2040 - accuracy: 0.9459 - val_loss: 0.1987 - val_accuracy: 0.9475\n",
      "Epoch 285/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2057 - accuracy: 0.9459 - val_loss: 0.1987 - val_accuracy: 0.9475\n",
      "Epoch 286/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2037 - accuracy: 0.9459 - val_loss: 0.1987 - val_accuracy: 0.9475\n",
      "Epoch 287/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2047 - accuracy: 0.9459 - val_loss: 0.2006 - val_accuracy: 0.9475\n",
      "Epoch 288/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2057 - accuracy: 0.9459 - val_loss: 0.1979 - val_accuracy: 0.9475\n",
      "Epoch 289/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2045 - accuracy: 0.9459 - val_loss: 0.1982 - val_accuracy: 0.9475\n",
      "Epoch 290/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2048 - accuracy: 0.9459 - val_loss: 0.1991 - val_accuracy: 0.9475\n",
      "Epoch 291/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2055 - accuracy: 0.9459 - val_loss: 0.1976 - val_accuracy: 0.9475\n",
      "Epoch 292/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2049 - accuracy: 0.9459 - val_loss: 0.1975 - val_accuracy: 0.9475\n",
      "Epoch 293/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2050 - accuracy: 0.9459 - val_loss: 0.1975 - val_accuracy: 0.9475\n",
      "Epoch 294/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2053 - accuracy: 0.9459 - val_loss: 0.1998 - val_accuracy: 0.9475\n",
      "Epoch 295/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2040 - accuracy: 0.9459 - val_loss: 0.2003 - val_accuracy: 0.9475\n",
      "Epoch 296/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2042 - accuracy: 0.9459 - val_loss: 0.1981 - val_accuracy: 0.9475\n",
      "Epoch 297/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2057 - accuracy: 0.9459 - val_loss: 0.1979 - val_accuracy: 0.9475\n",
      "Epoch 298/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2054 - accuracy: 0.9459 - val_loss: 0.1994 - val_accuracy: 0.9475\n",
      "Epoch 299/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2047 - accuracy: 0.9459 - val_loss: 0.1983 - val_accuracy: 0.9475\n",
      "Epoch 300/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2060 - accuracy: 0.9459 - val_loss: 0.1992 - val_accuracy: 0.9475\n",
      "Epoch 301/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2050 - accuracy: 0.9459 - val_loss: 0.1986 - val_accuracy: 0.9475\n",
      "Epoch 302/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2067 - accuracy: 0.9459 - val_loss: 0.1984 - val_accuracy: 0.9475\n",
      "Epoch 303/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2047 - accuracy: 0.9459 - val_loss: 0.1974 - val_accuracy: 0.9475\n",
      "Epoch 304/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2038 - accuracy: 0.9459 - val_loss: 0.1982 - val_accuracy: 0.9475\n",
      "Epoch 305/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2037 - accuracy: 0.9459 - val_loss: 0.2009 - val_accuracy: 0.9475\n",
      "Epoch 306/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2048 - accuracy: 0.9459 - val_loss: 0.1974 - val_accuracy: 0.9475\n",
      "Epoch 307/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2041 - accuracy: 0.9459 - val_loss: 0.1978 - val_accuracy: 0.9475\n",
      "Epoch 308/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2037 - accuracy: 0.9459 - val_loss: 0.2081 - val_accuracy: 0.9475\n",
      "Epoch 309/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2055 - accuracy: 0.9459 - val_loss: 0.1996 - val_accuracy: 0.9475\n",
      "Epoch 310/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2035 - accuracy: 0.9459 - val_loss: 0.1971 - val_accuracy: 0.9475\n",
      "Epoch 311/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2043 - accuracy: 0.9459 - val_loss: 0.1979 - val_accuracy: 0.9475\n",
      "Epoch 312/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2054 - accuracy: 0.9459 - val_loss: 0.2006 - val_accuracy: 0.9475\n",
      "Epoch 313/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2066 - accuracy: 0.9459 - val_loss: 0.1988 - val_accuracy: 0.9475\n",
      "Epoch 314/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2042 - accuracy: 0.9459 - val_loss: 0.2009 - val_accuracy: 0.9475\n",
      "Epoch 315/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2039 - accuracy: 0.9459 - val_loss: 0.1990 - val_accuracy: 0.9475\n",
      "Epoch 316/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2060 - accuracy: 0.9459 - val_loss: 0.1994 - val_accuracy: 0.9475\n",
      "Epoch 317/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2038 - accuracy: 0.9459 - val_loss: 0.2009 - val_accuracy: 0.9475\n",
      "Epoch 318/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2040 - accuracy: 0.9459 - val_loss: 0.1986 - val_accuracy: 0.9475\n",
      "Epoch 319/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2047 - accuracy: 0.9459 - val_loss: 0.2052 - val_accuracy: 0.9475\n",
      "Epoch 320/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2050 - accuracy: 0.9459 - val_loss: 0.1989 - val_accuracy: 0.9475\n",
      "Epoch 321/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2044 - accuracy: 0.9459 - val_loss: 0.1976 - val_accuracy: 0.9475\n",
      "Epoch 322/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2030 - accuracy: 0.9459 - val_loss: 0.1976 - val_accuracy: 0.9475\n",
      "Epoch 323/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2049 - accuracy: 0.9459 - val_loss: 0.1987 - val_accuracy: 0.9475\n",
      "Epoch 324/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2050 - accuracy: 0.9459 - val_loss: 0.1980 - val_accuracy: 0.9475\n",
      "Epoch 325/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2032 - accuracy: 0.9459 - val_loss: 0.1971 - val_accuracy: 0.9475\n",
      "Epoch 326/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2057 - accuracy: 0.9459 - val_loss: 0.1970 - val_accuracy: 0.9475\n",
      "Epoch 327/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2033 - accuracy: 0.9459 - val_loss: 0.1997 - val_accuracy: 0.9475\n",
      "Epoch 328/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2040 - accuracy: 0.9459 - val_loss: 0.1980 - val_accuracy: 0.9475\n",
      "Epoch 329/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2044 - accuracy: 0.9459 - val_loss: 0.1975 - val_accuracy: 0.9475\n",
      "Epoch 330/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2028 - accuracy: 0.9459 - val_loss: 0.1985 - val_accuracy: 0.9475\n",
      "Epoch 331/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2044 - accuracy: 0.9459 - val_loss: 0.1979 - val_accuracy: 0.9475\n",
      "Epoch 332/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2041 - accuracy: 0.9459 - val_loss: 0.1990 - val_accuracy: 0.9475\n",
      "Epoch 333/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2041 - accuracy: 0.9459 - val_loss: 0.1982 - val_accuracy: 0.9475\n",
      "Epoch 334/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2040 - accuracy: 0.9459 - val_loss: 0.1975 - val_accuracy: 0.9475\n",
      "Epoch 335/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2052 - accuracy: 0.9459 - val_loss: 0.2000 - val_accuracy: 0.9475\n",
      "Epoch 336/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2045 - accuracy: 0.9459 - val_loss: 0.1977 - val_accuracy: 0.9475\n",
      "Epoch 337/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2034 - accuracy: 0.9459 - val_loss: 0.1971 - val_accuracy: 0.9475\n",
      "Epoch 338/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2063 - accuracy: 0.9459 - val_loss: 0.1978 - val_accuracy: 0.9475\n",
      "Epoch 339/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2038 - accuracy: 0.9459 - val_loss: 0.2026 - val_accuracy: 0.9475\n",
      "Epoch 340/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2049 - accuracy: 0.9459 - val_loss: 0.1986 - val_accuracy: 0.9475\n",
      "Epoch 341/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2029 - accuracy: 0.9459 - val_loss: 0.1989 - val_accuracy: 0.9475\n",
      "Epoch 342/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2054 - accuracy: 0.9459 - val_loss: 0.1982 - val_accuracy: 0.9475\n",
      "Epoch 343/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2033 - accuracy: 0.9459 - val_loss: 0.1970 - val_accuracy: 0.9475\n",
      "Epoch 344/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2042 - accuracy: 0.9459 - val_loss: 0.1994 - val_accuracy: 0.9475\n",
      "Epoch 345/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2029 - accuracy: 0.9459 - val_loss: 0.1969 - val_accuracy: 0.9475\n",
      "Epoch 346/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2061 - accuracy: 0.9459 - val_loss: 0.1981 - val_accuracy: 0.9475\n",
      "Epoch 347/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2045 - accuracy: 0.9459 - val_loss: 0.1971 - val_accuracy: 0.9475\n",
      "Epoch 348/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2028 - accuracy: 0.9459 - val_loss: 0.1972 - val_accuracy: 0.9475\n",
      "Epoch 349/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2041 - accuracy: 0.9459 - val_loss: 0.1989 - val_accuracy: 0.9475\n",
      "Epoch 350/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2029 - accuracy: 0.9459 - val_loss: 0.1998 - val_accuracy: 0.9475\n",
      "Epoch 351/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2041 - accuracy: 0.9459 - val_loss: 0.1973 - val_accuracy: 0.9475\n",
      "Epoch 352/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2040 - accuracy: 0.9459 - val_loss: 0.1971 - val_accuracy: 0.9475\n",
      "Epoch 353/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2038 - accuracy: 0.9459 - val_loss: 0.1980 - val_accuracy: 0.9475\n",
      "Epoch 354/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2035 - accuracy: 0.9459 - val_loss: 0.1991 - val_accuracy: 0.9475\n",
      "Epoch 355/1000\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 0.2023 - accuracy: 0.9459 - val_loss: 0.1975 - val_accuracy: 0.9475\n",
      "Epoch 356/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2037 - accuracy: 0.9459 - val_loss: 0.1978 - val_accuracy: 0.9475\n",
      "Epoch 357/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2038 - accuracy: 0.9459 - val_loss: 0.1999 - val_accuracy: 0.9475\n",
      "Epoch 358/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2043 - accuracy: 0.9459 - val_loss: 0.1976 - val_accuracy: 0.9475\n",
      "Epoch 359/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2043 - accuracy: 0.9459 - val_loss: 0.1973 - val_accuracy: 0.9475\n",
      "Epoch 360/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2044 - accuracy: 0.9459 - val_loss: 0.1974 - val_accuracy: 0.9475\n",
      "Epoch 361/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2027 - accuracy: 0.9459 - val_loss: 0.1986 - val_accuracy: 0.9475\n",
      "Epoch 362/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2034 - accuracy: 0.9459 - val_loss: 0.1975 - val_accuracy: 0.9475\n",
      "Epoch 363/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2059 - accuracy: 0.9459 - val_loss: 0.1998 - val_accuracy: 0.9475\n",
      "Epoch 364/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2047 - accuracy: 0.9459 - val_loss: 0.1986 - val_accuracy: 0.9475\n",
      "Epoch 365/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2053 - accuracy: 0.9459 - val_loss: 0.1982 - val_accuracy: 0.9475\n",
      "Epoch 366/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2036 - accuracy: 0.9459 - val_loss: 0.1964 - val_accuracy: 0.9475\n",
      "Epoch 367/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2040 - accuracy: 0.9459 - val_loss: 0.1980 - val_accuracy: 0.9475\n",
      "Epoch 368/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2043 - accuracy: 0.9459 - val_loss: 0.1972 - val_accuracy: 0.9475\n",
      "Epoch 369/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2019 - accuracy: 0.9459 - val_loss: 0.2010 - val_accuracy: 0.9475\n",
      "Epoch 370/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2038 - accuracy: 0.9459 - val_loss: 0.1974 - val_accuracy: 0.9475\n",
      "Epoch 371/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2038 - accuracy: 0.9459 - val_loss: 0.1977 - val_accuracy: 0.9475\n",
      "Epoch 372/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2030 - accuracy: 0.9459 - val_loss: 0.1967 - val_accuracy: 0.9475\n",
      "Epoch 373/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2029 - accuracy: 0.9459 - val_loss: 0.1971 - val_accuracy: 0.9475\n",
      "Epoch 374/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2038 - accuracy: 0.9459 - val_loss: 0.1971 - val_accuracy: 0.9475\n",
      "Epoch 375/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2040 - accuracy: 0.9459 - val_loss: 0.1985 - val_accuracy: 0.9475\n",
      "Epoch 376/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2052 - accuracy: 0.9459 - val_loss: 0.1969 - val_accuracy: 0.9475\n",
      "Epoch 377/1000\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 0.2027 - accuracy: 0.9459 - val_loss: 0.2016 - val_accuracy: 0.9475\n",
      "Epoch 378/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2029 - accuracy: 0.9459 - val_loss: 0.1984 - val_accuracy: 0.9475\n",
      "Epoch 379/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2032 - accuracy: 0.9459 - val_loss: 0.1997 - val_accuracy: 0.9475\n",
      "Epoch 380/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2050 - accuracy: 0.9459 - val_loss: 0.1986 - val_accuracy: 0.9475\n",
      "Epoch 381/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2024 - accuracy: 0.9459 - val_loss: 0.1970 - val_accuracy: 0.9475\n",
      "Epoch 382/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2040 - accuracy: 0.9459 - val_loss: 0.1978 - val_accuracy: 0.9475\n",
      "Epoch 383/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2026 - accuracy: 0.9459 - val_loss: 0.1967 - val_accuracy: 0.9475\n",
      "Epoch 384/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2037 - accuracy: 0.9459 - val_loss: 0.2000 - val_accuracy: 0.9475\n",
      "Epoch 385/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2044 - accuracy: 0.9459 - val_loss: 0.1965 - val_accuracy: 0.9475\n",
      "Epoch 386/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2029 - accuracy: 0.9459 - val_loss: 0.1975 - val_accuracy: 0.9475\n",
      "Epoch 387/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2035 - accuracy: 0.9459 - val_loss: 0.1990 - val_accuracy: 0.9475\n",
      "Epoch 388/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2026 - accuracy: 0.9459 - val_loss: 0.2005 - val_accuracy: 0.9475\n",
      "Epoch 389/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2029 - accuracy: 0.9459 - val_loss: 0.2026 - val_accuracy: 0.9475\n",
      "Epoch 390/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2028 - accuracy: 0.9459 - val_loss: 0.1966 - val_accuracy: 0.9475\n",
      "Epoch 391/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2023 - accuracy: 0.9459 - val_loss: 0.2028 - val_accuracy: 0.9475\n",
      "Epoch 392/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2029 - accuracy: 0.9459 - val_loss: 0.1980 - val_accuracy: 0.9475\n",
      "Epoch 393/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2027 - accuracy: 0.9459 - val_loss: 0.2086 - val_accuracy: 0.9475\n",
      "Epoch 394/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2044 - accuracy: 0.9459 - val_loss: 0.1966 - val_accuracy: 0.9475\n",
      "Epoch 395/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2028 - accuracy: 0.9459 - val_loss: 0.1972 - val_accuracy: 0.9475\n",
      "Epoch 396/1000\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 0.2028 - accuracy: 0.9459 - val_loss: 0.1967 - val_accuracy: 0.9475\n",
      "Epoch 397/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2028 - accuracy: 0.9459 - val_loss: 0.1976 - val_accuracy: 0.9475\n",
      "Epoch 398/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2029 - accuracy: 0.9459 - val_loss: 0.1963 - val_accuracy: 0.9475\n",
      "Epoch 399/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2031 - accuracy: 0.9459 - val_loss: 0.1989 - val_accuracy: 0.9475\n",
      "Epoch 400/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2031 - accuracy: 0.9459 - val_loss: 0.1971 - val_accuracy: 0.9475\n",
      "Epoch 401/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2032 - accuracy: 0.9459 - val_loss: 0.1971 - val_accuracy: 0.9475\n",
      "Epoch 402/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2024 - accuracy: 0.9459 - val_loss: 0.2012 - val_accuracy: 0.9475\n",
      "Epoch 403/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2038 - accuracy: 0.9459 - val_loss: 0.1992 - val_accuracy: 0.9475\n",
      "Epoch 404/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2032 - accuracy: 0.9459 - val_loss: 0.1966 - val_accuracy: 0.9475\n",
      "Epoch 405/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2026 - accuracy: 0.9459 - val_loss: 0.1965 - val_accuracy: 0.9475\n",
      "Epoch 406/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2021 - accuracy: 0.9459 - val_loss: 0.1976 - val_accuracy: 0.9475\n",
      "Epoch 407/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2028 - accuracy: 0.9459 - val_loss: 0.1983 - val_accuracy: 0.9475\n",
      "Epoch 408/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2027 - accuracy: 0.9459 - val_loss: 0.1968 - val_accuracy: 0.9475\n",
      "Epoch 409/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2024 - accuracy: 0.9459 - val_loss: 0.1979 - val_accuracy: 0.9475\n",
      "Epoch 410/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2032 - accuracy: 0.9459 - val_loss: 0.1974 - val_accuracy: 0.9475\n",
      "Epoch 411/1000\n",
      "67/67 [==============================] - 13s 196ms/step - loss: 0.2032 - accuracy: 0.9459 - val_loss: 0.1969 - val_accuracy: 0.9475\n",
      "Epoch 412/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2026 - accuracy: 0.9459 - val_loss: 0.1963 - val_accuracy: 0.9475\n",
      "Epoch 413/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2045 - accuracy: 0.9459 - val_loss: 0.1967 - val_accuracy: 0.9475\n",
      "Epoch 414/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2021 - accuracy: 0.9459 - val_loss: 0.1984 - val_accuracy: 0.9475\n",
      "Epoch 415/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2031 - accuracy: 0.9459 - val_loss: 0.1990 - val_accuracy: 0.9475\n",
      "Epoch 416/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2029 - accuracy: 0.9459 - val_loss: 0.1972 - val_accuracy: 0.9475\n",
      "Epoch 417/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2021 - accuracy: 0.9459 - val_loss: 0.1974 - val_accuracy: 0.9475\n",
      "Epoch 418/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2025 - accuracy: 0.9459 - val_loss: 0.1968 - val_accuracy: 0.9475\n",
      "Epoch 419/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2017 - accuracy: 0.9459 - val_loss: 0.1964 - val_accuracy: 0.9475\n",
      "Epoch 420/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2027 - accuracy: 0.9459 - val_loss: 0.1986 - val_accuracy: 0.9475\n",
      "Epoch 421/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2041 - accuracy: 0.9459 - val_loss: 0.1991 - val_accuracy: 0.9475\n",
      "Epoch 422/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2040 - accuracy: 0.9459 - val_loss: 0.1969 - val_accuracy: 0.9475\n",
      "Epoch 423/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2031 - accuracy: 0.9459 - val_loss: 0.1977 - val_accuracy: 0.9475\n",
      "Epoch 424/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2021 - accuracy: 0.9459 - val_loss: 0.1973 - val_accuracy: 0.9475\n",
      "Epoch 425/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2019 - accuracy: 0.9459 - val_loss: 0.1969 - val_accuracy: 0.9475\n",
      "Epoch 426/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2021 - accuracy: 0.9459 - val_loss: 0.1970 - val_accuracy: 0.9475\n",
      "Epoch 427/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2028 - accuracy: 0.9459 - val_loss: 0.1970 - val_accuracy: 0.9475\n",
      "Epoch 428/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2015 - accuracy: 0.9459 - val_loss: 0.1960 - val_accuracy: 0.9475\n",
      "Epoch 429/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2038 - accuracy: 0.9459 - val_loss: 0.1976 - val_accuracy: 0.9475\n",
      "Epoch 430/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2028 - accuracy: 0.9459 - val_loss: 0.2065 - val_accuracy: 0.9475\n",
      "Epoch 431/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2033 - accuracy: 0.9459 - val_loss: 0.1970 - val_accuracy: 0.9475\n",
      "Epoch 432/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2023 - accuracy: 0.9459 - val_loss: 0.2019 - val_accuracy: 0.9475\n",
      "Epoch 433/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2015 - accuracy: 0.9459 - val_loss: 0.2047 - val_accuracy: 0.9475\n",
      "Epoch 434/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2024 - accuracy: 0.9459 - val_loss: 0.2006 - val_accuracy: 0.9475\n",
      "Epoch 435/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2030 - accuracy: 0.9459 - val_loss: 0.1957 - val_accuracy: 0.9475\n",
      "Epoch 436/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2019 - accuracy: 0.9459 - val_loss: 0.1969 - val_accuracy: 0.9475\n",
      "Epoch 437/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2025 - accuracy: 0.9459 - val_loss: 0.1969 - val_accuracy: 0.9475\n",
      "Epoch 438/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2027 - accuracy: 0.9459 - val_loss: 0.1970 - val_accuracy: 0.9475\n",
      "Epoch 439/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2018 - accuracy: 0.9459 - val_loss: 0.1963 - val_accuracy: 0.9475\n",
      "Epoch 440/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2028 - accuracy: 0.9459 - val_loss: 0.1973 - val_accuracy: 0.9475\n",
      "Epoch 441/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2020 - accuracy: 0.9459 - val_loss: 0.1973 - val_accuracy: 0.9475\n",
      "Epoch 442/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2024 - accuracy: 0.9459 - val_loss: 0.1963 - val_accuracy: 0.9475\n",
      "Epoch 443/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2016 - accuracy: 0.9459 - val_loss: 0.1959 - val_accuracy: 0.9475\n",
      "Epoch 444/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2026 - accuracy: 0.9459 - val_loss: 0.2014 - val_accuracy: 0.9475\n",
      "Epoch 445/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2052 - accuracy: 0.9459 - val_loss: 0.2048 - val_accuracy: 0.9475\n",
      "Epoch 446/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2031 - accuracy: 0.9459 - val_loss: 0.1959 - val_accuracy: 0.9475\n",
      "Epoch 447/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2011 - accuracy: 0.9459 - val_loss: 0.1977 - val_accuracy: 0.9475\n",
      "Epoch 448/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2016 - accuracy: 0.9459 - val_loss: 0.1982 - val_accuracy: 0.9475\n",
      "Epoch 449/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2019 - accuracy: 0.9459 - val_loss: 0.1991 - val_accuracy: 0.9475\n",
      "Epoch 450/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2028 - accuracy: 0.9459 - val_loss: 0.1977 - val_accuracy: 0.9475\n",
      "Epoch 451/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2041 - accuracy: 0.9459 - val_loss: 0.1969 - val_accuracy: 0.9475\n",
      "Epoch 452/1000\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 0.2026 - accuracy: 0.9459 - val_loss: 0.1957 - val_accuracy: 0.9475\n",
      "Epoch 453/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2023 - accuracy: 0.9459 - val_loss: 0.1960 - val_accuracy: 0.9475\n",
      "Epoch 454/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2039 - accuracy: 0.9459 - val_loss: 0.1988 - val_accuracy: 0.9475\n",
      "Epoch 455/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2019 - accuracy: 0.9459 - val_loss: 0.1960 - val_accuracy: 0.9475\n",
      "Epoch 456/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2012 - accuracy: 0.9459 - val_loss: 0.1975 - val_accuracy: 0.9475\n",
      "Epoch 457/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2030 - accuracy: 0.9459 - val_loss: 0.1969 - val_accuracy: 0.9475\n",
      "Epoch 458/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2024 - accuracy: 0.9459 - val_loss: 0.1954 - val_accuracy: 0.9475\n",
      "Epoch 459/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2027 - accuracy: 0.9459 - val_loss: 0.1959 - val_accuracy: 0.9475\n",
      "Epoch 460/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2028 - accuracy: 0.9459 - val_loss: 0.1975 - val_accuracy: 0.9475\n",
      "Epoch 461/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2026 - accuracy: 0.9459 - val_loss: 0.1969 - val_accuracy: 0.9475\n",
      "Epoch 462/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2020 - accuracy: 0.9459 - val_loss: 0.1961 - val_accuracy: 0.9475\n",
      "Epoch 463/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2021 - accuracy: 0.9459 - val_loss: 0.1959 - val_accuracy: 0.9475\n",
      "Epoch 464/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2027 - accuracy: 0.9459 - val_loss: 0.1973 - val_accuracy: 0.9475\n",
      "Epoch 465/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2024 - accuracy: 0.9459 - val_loss: 0.1963 - val_accuracy: 0.9475\n",
      "Epoch 466/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2042 - accuracy: 0.9459 - val_loss: 0.2032 - val_accuracy: 0.9475\n",
      "Epoch 467/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2031 - accuracy: 0.9459 - val_loss: 0.1962 - val_accuracy: 0.9475\n",
      "Epoch 468/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2025 - accuracy: 0.9459 - val_loss: 0.1960 - val_accuracy: 0.9475\n",
      "Epoch 469/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2022 - accuracy: 0.9459 - val_loss: 0.1959 - val_accuracy: 0.9475\n",
      "Epoch 470/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2024 - accuracy: 0.9459 - val_loss: 0.1995 - val_accuracy: 0.9475\n",
      "Epoch 471/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2019 - accuracy: 0.9459 - val_loss: 0.1968 - val_accuracy: 0.9475\n",
      "Epoch 472/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2019 - accuracy: 0.9459 - val_loss: 0.1968 - val_accuracy: 0.9475\n",
      "Epoch 473/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2021 - accuracy: 0.9459 - val_loss: 0.1968 - val_accuracy: 0.9475\n",
      "Epoch 474/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2006 - accuracy: 0.9459 - val_loss: 0.1965 - val_accuracy: 0.9475\n",
      "Epoch 475/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2017 - accuracy: 0.9459 - val_loss: 0.1983 - val_accuracy: 0.9475\n",
      "Epoch 476/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2024 - accuracy: 0.9459 - val_loss: 0.1967 - val_accuracy: 0.9475\n",
      "Epoch 477/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2022 - accuracy: 0.9459 - val_loss: 0.1962 - val_accuracy: 0.9475\n",
      "Epoch 478/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2035 - accuracy: 0.9459 - val_loss: 0.1968 - val_accuracy: 0.9475\n",
      "Epoch 479/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2031 - accuracy: 0.9459 - val_loss: 0.1975 - val_accuracy: 0.9475\n",
      "Epoch 480/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2033 - accuracy: 0.9459 - val_loss: 0.2122 - val_accuracy: 0.9475\n",
      "Epoch 481/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2037 - accuracy: 0.9459 - val_loss: 0.2000 - val_accuracy: 0.9475\n",
      "Epoch 482/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2015 - accuracy: 0.9459 - val_loss: 0.2011 - val_accuracy: 0.9475\n",
      "Epoch 483/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2030 - accuracy: 0.9459 - val_loss: 0.1994 - val_accuracy: 0.9475\n",
      "Epoch 484/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2017 - accuracy: 0.9459 - val_loss: 0.2020 - val_accuracy: 0.9475\n",
      "Epoch 485/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2017 - accuracy: 0.9459 - val_loss: 0.1961 - val_accuracy: 0.9475\n",
      "Epoch 486/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2025 - accuracy: 0.9459 - val_loss: 0.1965 - val_accuracy: 0.9475\n",
      "Epoch 487/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2028 - accuracy: 0.9459 - val_loss: 0.1959 - val_accuracy: 0.9475\n",
      "Epoch 488/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2020 - accuracy: 0.9459 - val_loss: 0.1963 - val_accuracy: 0.9475\n",
      "Epoch 489/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2015 - accuracy: 0.9459 - val_loss: 0.1977 - val_accuracy: 0.9475\n",
      "Epoch 490/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2029 - accuracy: 0.9459 - val_loss: 0.1964 - val_accuracy: 0.9475\n",
      "Epoch 491/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2029 - accuracy: 0.9459 - val_loss: 0.1968 - val_accuracy: 0.9475\n",
      "Epoch 492/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2020 - accuracy: 0.9459 - val_loss: 0.1994 - val_accuracy: 0.9475\n",
      "Epoch 493/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2023 - accuracy: 0.9459 - val_loss: 0.1985 - val_accuracy: 0.9475\n",
      "Epoch 494/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2027 - accuracy: 0.9459 - val_loss: 0.1955 - val_accuracy: 0.9475\n",
      "Epoch 495/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2008 - accuracy: 0.9459 - val_loss: 0.1953 - val_accuracy: 0.9475\n",
      "Epoch 496/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2025 - accuracy: 0.9459 - val_loss: 0.1969 - val_accuracy: 0.9475\n",
      "Epoch 497/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2030 - accuracy: 0.9459 - val_loss: 0.1966 - val_accuracy: 0.9475\n",
      "Epoch 498/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2026 - accuracy: 0.9459 - val_loss: 0.1981 - val_accuracy: 0.9475\n",
      "Epoch 499/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2021 - accuracy: 0.9459 - val_loss: 0.1956 - val_accuracy: 0.9475\n",
      "Epoch 500/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2019 - accuracy: 0.9459 - val_loss: 0.1966 - val_accuracy: 0.9475\n",
      "Epoch 501/1000\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 0.2016 - accuracy: 0.9459 - val_loss: 0.1964 - val_accuracy: 0.9475\n",
      "Epoch 502/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2040 - accuracy: 0.9459 - val_loss: 0.1961 - val_accuracy: 0.9475\n",
      "Epoch 503/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2029 - accuracy: 0.9459 - val_loss: 0.1962 - val_accuracy: 0.9475\n",
      "Epoch 504/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2025 - accuracy: 0.9459 - val_loss: 0.1964 - val_accuracy: 0.9475\n",
      "Epoch 505/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2023 - accuracy: 0.9459 - val_loss: 0.1965 - val_accuracy: 0.9475\n",
      "Epoch 506/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2023 - accuracy: 0.9459 - val_loss: 0.2011 - val_accuracy: 0.9475\n",
      "Epoch 507/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2017 - accuracy: 0.9459 - val_loss: 0.1969 - val_accuracy: 0.9475\n",
      "Epoch 508/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2031 - accuracy: 0.9459 - val_loss: 0.1960 - val_accuracy: 0.9475\n",
      "Epoch 509/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2031 - accuracy: 0.9459 - val_loss: 0.1961 - val_accuracy: 0.9475\n",
      "Epoch 510/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2025 - accuracy: 0.9459 - val_loss: 0.1998 - val_accuracy: 0.9475\n",
      "Epoch 511/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2035 - accuracy: 0.9459 - val_loss: 0.1958 - val_accuracy: 0.9475\n",
      "Epoch 512/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2013 - accuracy: 0.9459 - val_loss: 0.1959 - val_accuracy: 0.9475\n",
      "Epoch 513/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2029 - accuracy: 0.9459 - val_loss: 0.1957 - val_accuracy: 0.9475\n",
      "Epoch 514/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2031 - accuracy: 0.9459 - val_loss: 0.1969 - val_accuracy: 0.9475\n",
      "Epoch 515/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2042 - accuracy: 0.9459 - val_loss: 0.1971 - val_accuracy: 0.9475\n",
      "Epoch 516/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2023 - accuracy: 0.9459 - val_loss: 0.1954 - val_accuracy: 0.9475\n",
      "Epoch 517/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2016 - accuracy: 0.9459 - val_loss: 0.1974 - val_accuracy: 0.9475\n",
      "Epoch 518/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2033 - accuracy: 0.9459 - val_loss: 0.1999 - val_accuracy: 0.9475\n",
      "Epoch 519/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2040 - accuracy: 0.9459 - val_loss: 0.1964 - val_accuracy: 0.9475\n",
      "Epoch 520/1000\n",
      "67/67 [==============================] - 13s 198ms/step - loss: 0.2015 - accuracy: 0.9459 - val_loss: 0.1962 - val_accuracy: 0.9475\n",
      "Epoch 521/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2057 - accuracy: 0.9459 - val_loss: 0.1963 - val_accuracy: 0.9475\n",
      "Epoch 522/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2033 - accuracy: 0.9459 - val_loss: 0.1971 - val_accuracy: 0.9475\n",
      "Epoch 523/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2024 - accuracy: 0.9459 - val_loss: 0.1974 - val_accuracy: 0.9475\n",
      "Epoch 524/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2024 - accuracy: 0.9459 - val_loss: 0.1980 - val_accuracy: 0.9475\n",
      "Epoch 525/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2033 - accuracy: 0.9459 - val_loss: 0.1981 - val_accuracy: 0.9475\n",
      "Epoch 526/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2038 - accuracy: 0.9459 - val_loss: 0.1996 - val_accuracy: 0.9475\n",
      "Epoch 527/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2032 - accuracy: 0.9459 - val_loss: 0.1964 - val_accuracy: 0.9475\n",
      "Epoch 528/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2022 - accuracy: 0.9459 - val_loss: 0.1998 - val_accuracy: 0.9475\n",
      "Epoch 529/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2021 - accuracy: 0.9459 - val_loss: 0.1961 - val_accuracy: 0.9475\n",
      "Epoch 530/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2030 - accuracy: 0.9459 - val_loss: 0.1966 - val_accuracy: 0.9475\n",
      "Epoch 531/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2032 - accuracy: 0.9459 - val_loss: 0.1961 - val_accuracy: 0.9475\n",
      "Epoch 532/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2022 - accuracy: 0.9459 - val_loss: 0.1963 - val_accuracy: 0.9475\n",
      "Epoch 533/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2027 - accuracy: 0.9459 - val_loss: 0.1961 - val_accuracy: 0.9475\n",
      "Epoch 534/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2034 - accuracy: 0.9459 - val_loss: 0.1961 - val_accuracy: 0.9475\n",
      "Epoch 535/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2023 - accuracy: 0.9459 - val_loss: 0.1957 - val_accuracy: 0.9475\n",
      "Epoch 536/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2032 - accuracy: 0.9459 - val_loss: 0.1957 - val_accuracy: 0.9475\n",
      "Epoch 537/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2032 - accuracy: 0.9459 - val_loss: 0.1970 - val_accuracy: 0.9475\n",
      "Epoch 538/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2035 - accuracy: 0.9459 - val_loss: 0.1957 - val_accuracy: 0.9475\n",
      "Epoch 539/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2021 - accuracy: 0.9459 - val_loss: 0.1963 - val_accuracy: 0.9475\n",
      "Epoch 540/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2034 - accuracy: 0.9459 - val_loss: 0.2083 - val_accuracy: 0.9475\n",
      "Epoch 541/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2038 - accuracy: 0.9459 - val_loss: 0.1964 - val_accuracy: 0.9475\n",
      "Epoch 542/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2030 - accuracy: 0.9459 - val_loss: 0.1955 - val_accuracy: 0.9475\n",
      "Epoch 543/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2018 - accuracy: 0.9459 - val_loss: 0.1959 - val_accuracy: 0.9475\n",
      "Epoch 544/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2022 - accuracy: 0.9459 - val_loss: 0.2009 - val_accuracy: 0.9475\n",
      "Epoch 545/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2012 - accuracy: 0.9459 - val_loss: 0.1966 - val_accuracy: 0.9475\n",
      "Epoch 546/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2032 - accuracy: 0.9459 - val_loss: 0.1972 - val_accuracy: 0.9475\n",
      "Epoch 547/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2020 - accuracy: 0.9459 - val_loss: 0.1974 - val_accuracy: 0.9475\n",
      "Epoch 548/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2025 - accuracy: 0.9459 - val_loss: 0.1967 - val_accuracy: 0.9475\n",
      "Epoch 549/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2032 - accuracy: 0.9459 - val_loss: 0.1963 - val_accuracy: 0.9475\n",
      "Epoch 550/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2008 - accuracy: 0.9459 - val_loss: 0.1969 - val_accuracy: 0.9475\n",
      "Epoch 551/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2026 - accuracy: 0.9459 - val_loss: 0.1968 - val_accuracy: 0.9475\n",
      "Epoch 552/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2026 - accuracy: 0.9459 - val_loss: 0.1958 - val_accuracy: 0.9475\n",
      "Epoch 553/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2022 - accuracy: 0.9459 - val_loss: 0.1954 - val_accuracy: 0.9475\n",
      "Epoch 554/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2009 - accuracy: 0.9459 - val_loss: 0.1972 - val_accuracy: 0.9475\n",
      "Epoch 555/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2024 - accuracy: 0.9459 - val_loss: 0.1977 - val_accuracy: 0.9475\n",
      "Epoch 556/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2021 - accuracy: 0.9459 - val_loss: 0.1952 - val_accuracy: 0.9475\n",
      "Epoch 557/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2027 - accuracy: 0.9459 - val_loss: 0.1988 - val_accuracy: 0.9475\n",
      "Epoch 558/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2022 - accuracy: 0.9459 - val_loss: 0.2005 - val_accuracy: 0.9475\n",
      "Epoch 559/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2024 - accuracy: 0.9459 - val_loss: 0.1999 - val_accuracy: 0.9475\n",
      "Epoch 560/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2045 - accuracy: 0.9459 - val_loss: 0.1982 - val_accuracy: 0.9475\n",
      "Epoch 561/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2034 - accuracy: 0.9459 - val_loss: 0.2001 - val_accuracy: 0.9475\n",
      "Epoch 562/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2015 - accuracy: 0.9459 - val_loss: 0.1964 - val_accuracy: 0.9475\n",
      "Epoch 563/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2025 - accuracy: 0.9459 - val_loss: 0.1957 - val_accuracy: 0.9475\n",
      "Epoch 564/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2013 - accuracy: 0.9459 - val_loss: 0.1947 - val_accuracy: 0.9475\n",
      "Epoch 565/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2016 - accuracy: 0.9459 - val_loss: 0.1968 - val_accuracy: 0.9475\n",
      "Epoch 566/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2018 - accuracy: 0.9459 - val_loss: 0.1973 - val_accuracy: 0.9475\n",
      "Epoch 567/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2029 - accuracy: 0.9459 - val_loss: 0.1952 - val_accuracy: 0.9475\n",
      "Epoch 568/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2036 - accuracy: 0.9459 - val_loss: 0.1963 - val_accuracy: 0.9475\n",
      "Epoch 569/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2010 - accuracy: 0.9459 - val_loss: 0.2008 - val_accuracy: 0.9475\n",
      "Epoch 570/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2050 - accuracy: 0.9459 - val_loss: 0.2011 - val_accuracy: 0.9475\n",
      "Epoch 571/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2042 - accuracy: 0.9459 - val_loss: 0.1968 - val_accuracy: 0.9475\n",
      "Epoch 572/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2024 - accuracy: 0.9459 - val_loss: 0.1955 - val_accuracy: 0.9475\n",
      "Epoch 573/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2019 - accuracy: 0.9459 - val_loss: 0.1970 - val_accuracy: 0.9475\n",
      "Epoch 574/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2027 - accuracy: 0.9459 - val_loss: 0.1950 - val_accuracy: 0.9475\n",
      "Epoch 575/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2051 - accuracy: 0.9459 - val_loss: 0.1964 - val_accuracy: 0.9475\n",
      "Epoch 576/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2020 - accuracy: 0.9459 - val_loss: 0.1961 - val_accuracy: 0.9475\n",
      "Epoch 577/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2030 - accuracy: 0.9459 - val_loss: 0.1981 - val_accuracy: 0.9475\n",
      "Epoch 578/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2026 - accuracy: 0.9459 - val_loss: 0.1981 - val_accuracy: 0.9475\n",
      "Epoch 579/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2022 - accuracy: 0.9459 - val_loss: 0.1976 - val_accuracy: 0.9475\n",
      "Epoch 580/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2020 - accuracy: 0.9459 - val_loss: 0.1963 - val_accuracy: 0.9475\n",
      "Epoch 581/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2023 - accuracy: 0.9459 - val_loss: 0.2020 - val_accuracy: 0.9475\n",
      "Epoch 582/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2020 - accuracy: 0.9459 - val_loss: 0.1952 - val_accuracy: 0.9475\n",
      "Epoch 583/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2045 - accuracy: 0.9459 - val_loss: 0.1967 - val_accuracy: 0.9475\n",
      "Epoch 584/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2017 - accuracy: 0.9459 - val_loss: 0.1955 - val_accuracy: 0.9475\n",
      "Epoch 585/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2011 - accuracy: 0.9459 - val_loss: 0.1966 - val_accuracy: 0.9475\n",
      "Epoch 586/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2023 - accuracy: 0.9459 - val_loss: 0.1955 - val_accuracy: 0.9475\n",
      "Epoch 587/1000\n",
      "67/67 [==============================] - 13s 196ms/step - loss: 0.2033 - accuracy: 0.9459 - val_loss: 0.1960 - val_accuracy: 0.9475\n",
      "Epoch 588/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2013 - accuracy: 0.9459 - val_loss: 0.1951 - val_accuracy: 0.9475\n",
      "Epoch 589/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2010 - accuracy: 0.9459 - val_loss: 0.1957 - val_accuracy: 0.9475\n",
      "Epoch 590/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2025 - accuracy: 0.9459 - val_loss: 0.1984 - val_accuracy: 0.9475\n",
      "Epoch 591/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2008 - accuracy: 0.9459 - val_loss: 0.1951 - val_accuracy: 0.9475\n",
      "Epoch 592/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2027 - accuracy: 0.9459 - val_loss: 0.1961 - val_accuracy: 0.9475\n",
      "Epoch 593/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2024 - accuracy: 0.9459 - val_loss: 0.1980 - val_accuracy: 0.9475\n",
      "Epoch 594/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2024 - accuracy: 0.9459 - val_loss: 0.2002 - val_accuracy: 0.9475\n",
      "Epoch 595/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2028 - accuracy: 0.9459 - val_loss: 0.1964 - val_accuracy: 0.9475\n",
      "Epoch 596/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2016 - accuracy: 0.9459 - val_loss: 0.1953 - val_accuracy: 0.9475\n",
      "Epoch 597/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2001 - accuracy: 0.9459 - val_loss: 0.1966 - val_accuracy: 0.9475\n",
      "Epoch 598/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2016 - accuracy: 0.9459 - val_loss: 0.1960 - val_accuracy: 0.9475\n",
      "Epoch 599/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2018 - accuracy: 0.9459 - val_loss: 0.1947 - val_accuracy: 0.9475\n",
      "Epoch 600/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2016 - accuracy: 0.9459 - val_loss: 0.1987 - val_accuracy: 0.9475\n",
      "Epoch 601/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2018 - accuracy: 0.9459 - val_loss: 0.1970 - val_accuracy: 0.9475\n",
      "Epoch 602/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2025 - accuracy: 0.9459 - val_loss: 0.1949 - val_accuracy: 0.9475\n",
      "Epoch 603/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2024 - accuracy: 0.9459 - val_loss: 0.2083 - val_accuracy: 0.9475\n",
      "Epoch 604/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2018 - accuracy: 0.9459 - val_loss: 0.1948 - val_accuracy: 0.9475\n",
      "Epoch 605/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2024 - accuracy: 0.9459 - val_loss: 0.1951 - val_accuracy: 0.9475\n",
      "Epoch 606/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2022 - accuracy: 0.9459 - val_loss: 0.1968 - val_accuracy: 0.9475\n",
      "Epoch 607/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2010 - accuracy: 0.9459 - val_loss: 0.1971 - val_accuracy: 0.9475\n",
      "Epoch 608/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2013 - accuracy: 0.9459 - val_loss: 0.1945 - val_accuracy: 0.9475\n",
      "Epoch 609/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2030 - accuracy: 0.9459 - val_loss: 0.1962 - val_accuracy: 0.9475\n",
      "Epoch 610/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2009 - accuracy: 0.9459 - val_loss: 0.1947 - val_accuracy: 0.9475\n",
      "Epoch 611/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2034 - accuracy: 0.9459 - val_loss: 0.1966 - val_accuracy: 0.9475\n",
      "Epoch 612/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2011 - accuracy: 0.9459 - val_loss: 0.1951 - val_accuracy: 0.9475\n",
      "Epoch 613/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2015 - accuracy: 0.9459 - val_loss: 0.1990 - val_accuracy: 0.9475\n",
      "Epoch 614/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2017 - accuracy: 0.9459 - val_loss: 0.2030 - val_accuracy: 0.9475\n",
      "Epoch 615/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2013 - accuracy: 0.9459 - val_loss: 0.1974 - val_accuracy: 0.9475\n",
      "Epoch 616/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2012 - accuracy: 0.9459 - val_loss: 0.1950 - val_accuracy: 0.9475\n",
      "Epoch 617/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2028 - accuracy: 0.9459 - val_loss: 0.1956 - val_accuracy: 0.9475\n",
      "Epoch 618/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2018 - accuracy: 0.9459 - val_loss: 0.1952 - val_accuracy: 0.9475\n",
      "Epoch 619/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2031 - accuracy: 0.9459 - val_loss: 0.1953 - val_accuracy: 0.9475\n",
      "Epoch 620/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2013 - accuracy: 0.9459 - val_loss: 0.1949 - val_accuracy: 0.9475\n",
      "Epoch 621/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2010 - accuracy: 0.9459 - val_loss: 0.1957 - val_accuracy: 0.9475\n",
      "Epoch 622/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2026 - accuracy: 0.9459 - val_loss: 0.1989 - val_accuracy: 0.9475\n",
      "Epoch 623/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2025 - accuracy: 0.9459 - val_loss: 0.1950 - val_accuracy: 0.9475\n",
      "Epoch 624/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2015 - accuracy: 0.9459 - val_loss: 0.1949 - val_accuracy: 0.9475\n",
      "Epoch 625/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2007 - accuracy: 0.9459 - val_loss: 0.1960 - val_accuracy: 0.9475\n",
      "Epoch 626/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2019 - accuracy: 0.9459 - val_loss: 0.1952 - val_accuracy: 0.9475\n",
      "Epoch 627/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2030 - accuracy: 0.9459 - val_loss: 0.1979 - val_accuracy: 0.9475\n",
      "Epoch 628/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2019 - accuracy: 0.9459 - val_loss: 0.1948 - val_accuracy: 0.9475\n",
      "Epoch 629/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2025 - accuracy: 0.9459 - val_loss: 0.1985 - val_accuracy: 0.9475\n",
      "Epoch 630/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2019 - accuracy: 0.9459 - val_loss: 0.2019 - val_accuracy: 0.9475\n",
      "Epoch 631/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2014 - accuracy: 0.9459 - val_loss: 0.1970 - val_accuracy: 0.9475\n",
      "Epoch 632/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2036 - accuracy: 0.9459 - val_loss: 0.1965 - val_accuracy: 0.9475\n",
      "Epoch 633/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2013 - accuracy: 0.9459 - val_loss: 0.1980 - val_accuracy: 0.9475\n",
      "Epoch 634/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2011 - accuracy: 0.9459 - val_loss: 0.1951 - val_accuracy: 0.9475\n",
      "Epoch 635/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2011 - accuracy: 0.9459 - val_loss: 0.1962 - val_accuracy: 0.9475\n",
      "Epoch 636/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2021 - accuracy: 0.9459 - val_loss: 0.1956 - val_accuracy: 0.9475\n",
      "Epoch 637/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2025 - accuracy: 0.9459 - val_loss: 0.1975 - val_accuracy: 0.9475\n",
      "Epoch 638/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2017 - accuracy: 0.9459 - val_loss: 0.1950 - val_accuracy: 0.9475\n",
      "Epoch 639/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2011 - accuracy: 0.9459 - val_loss: 0.1946 - val_accuracy: 0.9475\n",
      "Epoch 640/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2012 - accuracy: 0.9459 - val_loss: 0.1956 - val_accuracy: 0.9475\n",
      "Epoch 641/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2005 - accuracy: 0.9459 - val_loss: 0.1957 - val_accuracy: 0.9475\n",
      "Epoch 642/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2019 - accuracy: 0.9459 - val_loss: 0.1952 - val_accuracy: 0.9475\n",
      "Epoch 643/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2012 - accuracy: 0.9459 - val_loss: 0.1948 - val_accuracy: 0.9475\n",
      "Epoch 644/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2025 - accuracy: 0.9459 - val_loss: 0.1954 - val_accuracy: 0.9475\n",
      "Epoch 645/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2036 - accuracy: 0.9459 - val_loss: 0.1955 - val_accuracy: 0.9475\n",
      "Epoch 646/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2020 - accuracy: 0.9459 - val_loss: 0.1952 - val_accuracy: 0.9475\n",
      "Epoch 647/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2023 - accuracy: 0.9459 - val_loss: 0.1954 - val_accuracy: 0.9475\n",
      "Epoch 648/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2017 - accuracy: 0.9459 - val_loss: 0.1950 - val_accuracy: 0.9475\n",
      "Epoch 649/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2026 - accuracy: 0.9459 - val_loss: 0.1952 - val_accuracy: 0.9475\n",
      "Epoch 650/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2017 - accuracy: 0.9459 - val_loss: 0.1947 - val_accuracy: 0.9475\n",
      "Epoch 651/1000\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 0.2011 - accuracy: 0.9459 - val_loss: 0.2022 - val_accuracy: 0.9475\n",
      "Epoch 652/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2011 - accuracy: 0.9459 - val_loss: 0.1950 - val_accuracy: 0.9475\n",
      "Epoch 653/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2035 - accuracy: 0.9459 - val_loss: 0.1951 - val_accuracy: 0.9475\n",
      "Epoch 654/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2046 - accuracy: 0.9459 - val_loss: 0.1973 - val_accuracy: 0.9475\n",
      "Epoch 655/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2020 - accuracy: 0.9459 - val_loss: 0.1972 - val_accuracy: 0.9475\n",
      "Epoch 656/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2036 - accuracy: 0.9459 - val_loss: 0.1965 - val_accuracy: 0.9475\n",
      "Epoch 657/1000\n",
      "67/67 [==============================] - 13s 197ms/step - loss: 0.2040 - accuracy: 0.9459 - val_loss: 0.1959 - val_accuracy: 0.9475\n",
      "Epoch 658/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2035 - accuracy: 0.9459 - val_loss: 0.1959 - val_accuracy: 0.9475\n",
      "Epoch 659/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2022 - accuracy: 0.9459 - val_loss: 0.1967 - val_accuracy: 0.9475\n",
      "Epoch 660/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2009 - accuracy: 0.9459 - val_loss: 0.1959 - val_accuracy: 0.9475\n",
      "Epoch 661/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2021 - accuracy: 0.9459 - val_loss: 0.1970 - val_accuracy: 0.9475\n",
      "Epoch 662/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2019 - accuracy: 0.9459 - val_loss: 0.1961 - val_accuracy: 0.9475\n",
      "Epoch 663/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2014 - accuracy: 0.9459 - val_loss: 0.1963 - val_accuracy: 0.9475\n",
      "Epoch 664/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2024 - accuracy: 0.9459 - val_loss: 0.1954 - val_accuracy: 0.9475\n",
      "Epoch 665/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2023 - accuracy: 0.9459 - val_loss: 0.1995 - val_accuracy: 0.9475\n",
      "Epoch 666/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2034 - accuracy: 0.9459 - val_loss: 0.1975 - val_accuracy: 0.9475\n",
      "Epoch 667/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2013 - accuracy: 0.9459 - val_loss: 0.1953 - val_accuracy: 0.9475\n",
      "Epoch 668/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2025 - accuracy: 0.9459 - val_loss: 0.1963 - val_accuracy: 0.9475\n",
      "Epoch 669/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2004 - accuracy: 0.9459 - val_loss: 0.1953 - val_accuracy: 0.9475\n",
      "Epoch 670/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2037 - accuracy: 0.9459 - val_loss: 0.1963 - val_accuracy: 0.9475\n",
      "Epoch 671/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2027 - accuracy: 0.9459 - val_loss: 0.1961 - val_accuracy: 0.9475\n",
      "Epoch 672/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2006 - accuracy: 0.9459 - val_loss: 0.1947 - val_accuracy: 0.9475\n",
      "Epoch 673/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2023 - accuracy: 0.9459 - val_loss: 0.2003 - val_accuracy: 0.9475\n",
      "Epoch 674/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2011 - accuracy: 0.9459 - val_loss: 0.1960 - val_accuracy: 0.9475\n",
      "Epoch 675/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2025 - accuracy: 0.9459 - val_loss: 0.1948 - val_accuracy: 0.9475\n",
      "Epoch 676/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2009 - accuracy: 0.9459 - val_loss: 0.1987 - val_accuracy: 0.9475\n",
      "Epoch 677/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2026 - accuracy: 0.9459 - val_loss: 0.1955 - val_accuracy: 0.9475\n",
      "Epoch 678/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2028 - accuracy: 0.9459 - val_loss: 0.1944 - val_accuracy: 0.9475\n",
      "Epoch 679/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2014 - accuracy: 0.9459 - val_loss: 0.1956 - val_accuracy: 0.9475\n",
      "Epoch 680/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2017 - accuracy: 0.9459 - val_loss: 0.1954 - val_accuracy: 0.9475\n",
      "Epoch 681/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2022 - accuracy: 0.9459 - val_loss: 0.1951 - val_accuracy: 0.9475\n",
      "Epoch 682/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2016 - accuracy: 0.9459 - val_loss: 0.1949 - val_accuracy: 0.9475\n",
      "Epoch 683/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2044 - accuracy: 0.9459 - val_loss: 0.1979 - val_accuracy: 0.9475\n",
      "Epoch 684/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2029 - accuracy: 0.9459 - val_loss: 0.1955 - val_accuracy: 0.9475\n",
      "Epoch 685/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2030 - accuracy: 0.9459 - val_loss: 0.1949 - val_accuracy: 0.9475\n",
      "Epoch 686/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2015 - accuracy: 0.9459 - val_loss: 0.1944 - val_accuracy: 0.9475\n",
      "Epoch 687/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2023 - accuracy: 0.9459 - val_loss: 0.1958 - val_accuracy: 0.9475\n",
      "Epoch 688/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2009 - accuracy: 0.9459 - val_loss: 0.1956 - val_accuracy: 0.9475\n",
      "Epoch 689/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2007 - accuracy: 0.9459 - val_loss: 0.1944 - val_accuracy: 0.9475\n",
      "Epoch 690/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2014 - accuracy: 0.9459 - val_loss: 0.1945 - val_accuracy: 0.9475\n",
      "Epoch 691/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2015 - accuracy: 0.9459 - val_loss: 0.1979 - val_accuracy: 0.9475\n",
      "Epoch 692/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2020 - accuracy: 0.9459 - val_loss: 0.1967 - val_accuracy: 0.9475\n",
      "Epoch 693/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2025 - accuracy: 0.9459 - val_loss: 0.1969 - val_accuracy: 0.9475\n",
      "Epoch 694/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2019 - accuracy: 0.9459 - val_loss: 0.1940 - val_accuracy: 0.9475\n",
      "Epoch 695/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2014 - accuracy: 0.9459 - val_loss: 0.1940 - val_accuracy: 0.9475\n",
      "Epoch 696/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2028 - accuracy: 0.9459 - val_loss: 0.1944 - val_accuracy: 0.9475\n",
      "Epoch 697/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2021 - accuracy: 0.9459 - val_loss: 0.1942 - val_accuracy: 0.9475\n",
      "Epoch 698/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2014 - accuracy: 0.9459 - val_loss: 0.1943 - val_accuracy: 0.9475\n",
      "Epoch 699/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2012 - accuracy: 0.9459 - val_loss: 0.1976 - val_accuracy: 0.9475\n",
      "Epoch 700/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2009 - accuracy: 0.9459 - val_loss: 0.1948 - val_accuracy: 0.9475\n",
      "Epoch 701/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2013 - accuracy: 0.9459 - val_loss: 0.1959 - val_accuracy: 0.9475\n",
      "Epoch 702/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2060 - accuracy: 0.9459 - val_loss: 0.2013 - val_accuracy: 0.9475\n",
      "Epoch 703/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2021 - accuracy: 0.9459 - val_loss: 0.1949 - val_accuracy: 0.9475\n",
      "Epoch 704/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2016 - accuracy: 0.9459 - val_loss: 0.1945 - val_accuracy: 0.9475\n",
      "Epoch 705/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2011 - accuracy: 0.9459 - val_loss: 0.1947 - val_accuracy: 0.9475\n",
      "Epoch 706/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2029 - accuracy: 0.9459 - val_loss: 0.1965 - val_accuracy: 0.9475\n",
      "Epoch 707/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2008 - accuracy: 0.9459 - val_loss: 0.1952 - val_accuracy: 0.9475\n",
      "Epoch 708/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2020 - accuracy: 0.9459 - val_loss: 0.1952 - val_accuracy: 0.9475\n",
      "Epoch 709/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2011 - accuracy: 0.9459 - val_loss: 0.1947 - val_accuracy: 0.9475\n",
      "Epoch 710/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2002 - accuracy: 0.9459 - val_loss: 0.1945 - val_accuracy: 0.9475\n",
      "Epoch 711/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2016 - accuracy: 0.9459 - val_loss: 0.1942 - val_accuracy: 0.9475\n",
      "Epoch 712/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2027 - accuracy: 0.9459 - val_loss: 0.1952 - val_accuracy: 0.9475\n",
      "Epoch 713/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2023 - accuracy: 0.9459 - val_loss: 0.1964 - val_accuracy: 0.9475\n",
      "Epoch 714/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2033 - accuracy: 0.9459 - val_loss: 0.1946 - val_accuracy: 0.9475\n",
      "Epoch 715/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2019 - accuracy: 0.9459 - val_loss: 0.1950 - val_accuracy: 0.9475\n",
      "Epoch 716/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2023 - accuracy: 0.9459 - val_loss: 0.1974 - val_accuracy: 0.9475\n",
      "Epoch 717/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2016 - accuracy: 0.9459 - val_loss: 0.1946 - val_accuracy: 0.9475\n",
      "Epoch 718/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2014 - accuracy: 0.9459 - val_loss: 0.1945 - val_accuracy: 0.9475\n",
      "Epoch 719/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2010 - accuracy: 0.9459 - val_loss: 0.1951 - val_accuracy: 0.9475\n",
      "Epoch 720/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2013 - accuracy: 0.9459 - val_loss: 0.1942 - val_accuracy: 0.9475\n",
      "Epoch 721/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2018 - accuracy: 0.9459 - val_loss: 0.1939 - val_accuracy: 0.9475\n",
      "Epoch 722/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2010 - accuracy: 0.9459 - val_loss: 0.1958 - val_accuracy: 0.9475\n",
      "Epoch 723/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2010 - accuracy: 0.9459 - val_loss: 0.1946 - val_accuracy: 0.9475\n",
      "Epoch 724/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2004 - accuracy: 0.9459 - val_loss: 0.1961 - val_accuracy: 0.9475\n",
      "Epoch 725/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2019 - accuracy: 0.9459 - val_loss: 0.1945 - val_accuracy: 0.9475\n",
      "Epoch 726/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2017 - accuracy: 0.9459 - val_loss: 0.1943 - val_accuracy: 0.9475\n",
      "Epoch 727/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2015 - accuracy: 0.9459 - val_loss: 0.1942 - val_accuracy: 0.9475\n",
      "Epoch 728/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2012 - accuracy: 0.9459 - val_loss: 0.1958 - val_accuracy: 0.9475\n",
      "Epoch 729/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2018 - accuracy: 0.9459 - val_loss: 0.1944 - val_accuracy: 0.9475\n",
      "Epoch 730/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2010 - accuracy: 0.9459 - val_loss: 0.1938 - val_accuracy: 0.9475\n",
      "Epoch 731/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2023 - accuracy: 0.9459 - val_loss: 0.1948 - val_accuracy: 0.9475\n",
      "Epoch 732/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2021 - accuracy: 0.9459 - val_loss: 0.1972 - val_accuracy: 0.9475\n",
      "Epoch 733/1000\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 0.2024 - accuracy: 0.9459 - val_loss: 0.1941 - val_accuracy: 0.9475\n",
      "Epoch 734/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2014 - accuracy: 0.9459 - val_loss: 0.2003 - val_accuracy: 0.9475\n",
      "Epoch 735/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2021 - accuracy: 0.9459 - val_loss: 0.1937 - val_accuracy: 0.9475\n",
      "Epoch 736/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2019 - accuracy: 0.9459 - val_loss: 0.1946 - val_accuracy: 0.9475\n",
      "Epoch 737/1000\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 0.2041 - accuracy: 0.9459 - val_loss: 0.1954 - val_accuracy: 0.9475\n",
      "Epoch 738/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2020 - accuracy: 0.9459 - val_loss: 0.1937 - val_accuracy: 0.9475\n",
      "Epoch 739/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2012 - accuracy: 0.9459 - val_loss: 0.1961 - val_accuracy: 0.9475\n",
      "Epoch 740/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2029 - accuracy: 0.9459 - val_loss: 0.1977 - val_accuracy: 0.9475\n",
      "Epoch 741/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2018 - accuracy: 0.9459 - val_loss: 0.1944 - val_accuracy: 0.9475\n",
      "Epoch 742/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2012 - accuracy: 0.9459 - val_loss: 0.1947 - val_accuracy: 0.9475\n",
      "Epoch 743/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2030 - accuracy: 0.9459 - val_loss: 0.1950 - val_accuracy: 0.9475\n",
      "Epoch 744/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2008 - accuracy: 0.9459 - val_loss: 0.1958 - val_accuracy: 0.9475\n",
      "Epoch 745/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2015 - accuracy: 0.9459 - val_loss: 0.1943 - val_accuracy: 0.9475\n",
      "Epoch 746/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2009 - accuracy: 0.9459 - val_loss: 0.1945 - val_accuracy: 0.9475\n",
      "Epoch 747/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2005 - accuracy: 0.9459 - val_loss: 0.1938 - val_accuracy: 0.9475\n",
      "Epoch 748/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2025 - accuracy: 0.9459 - val_loss: 0.1946 - val_accuracy: 0.9475\n",
      "Epoch 749/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2011 - accuracy: 0.9459 - val_loss: 0.1934 - val_accuracy: 0.9475\n",
      "Epoch 750/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2017 - accuracy: 0.9459 - val_loss: 0.1982 - val_accuracy: 0.9475\n",
      "Epoch 751/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2022 - accuracy: 0.9459 - val_loss: 0.1950 - val_accuracy: 0.9475\n",
      "Epoch 752/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2021 - accuracy: 0.9459 - val_loss: 0.1942 - val_accuracy: 0.9475\n",
      "Epoch 753/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2005 - accuracy: 0.9459 - val_loss: 0.1934 - val_accuracy: 0.9475\n",
      "Epoch 754/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2010 - accuracy: 0.9459 - val_loss: 0.1941 - val_accuracy: 0.9475\n",
      "Epoch 755/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2015 - accuracy: 0.9459 - val_loss: 0.1940 - val_accuracy: 0.9475\n",
      "Epoch 756/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2007 - accuracy: 0.9459 - val_loss: 0.1939 - val_accuracy: 0.9475\n",
      "Epoch 757/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2025 - accuracy: 0.9459 - val_loss: 0.1949 - val_accuracy: 0.9475\n",
      "Epoch 758/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2012 - accuracy: 0.9459 - val_loss: 0.1944 - val_accuracy: 0.9475\n",
      "Epoch 759/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2014 - accuracy: 0.9459 - val_loss: 0.1939 - val_accuracy: 0.9475\n",
      "Epoch 760/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2002 - accuracy: 0.9459 - val_loss: 0.1931 - val_accuracy: 0.9475\n",
      "Epoch 761/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2005 - accuracy: 0.9459 - val_loss: 0.1954 - val_accuracy: 0.9475\n",
      "Epoch 762/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2010 - accuracy: 0.9459 - val_loss: 0.1957 - val_accuracy: 0.9475\n",
      "Epoch 763/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2024 - accuracy: 0.9459 - val_loss: 0.1944 - val_accuracy: 0.9475\n",
      "Epoch 764/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2018 - accuracy: 0.9459 - val_loss: 0.1944 - val_accuracy: 0.9475\n",
      "Epoch 765/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2036 - accuracy: 0.9459 - val_loss: 0.1941 - val_accuracy: 0.9475\n",
      "Epoch 766/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2014 - accuracy: 0.9459 - val_loss: 0.1948 - val_accuracy: 0.9475\n",
      "Epoch 767/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2006 - accuracy: 0.9459 - val_loss: 0.1984 - val_accuracy: 0.9475\n",
      "Epoch 768/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2007 - accuracy: 0.9459 - val_loss: 0.1938 - val_accuracy: 0.9475\n",
      "Epoch 769/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2010 - accuracy: 0.9459 - val_loss: 0.1944 - val_accuracy: 0.9475\n",
      "Epoch 770/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2027 - accuracy: 0.9459 - val_loss: 0.2000 - val_accuracy: 0.9475\n",
      "Epoch 771/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2011 - accuracy: 0.9459 - val_loss: 0.1934 - val_accuracy: 0.9475\n",
      "Epoch 772/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2001 - accuracy: 0.9459 - val_loss: 0.1987 - val_accuracy: 0.9475\n",
      "Epoch 773/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2009 - accuracy: 0.9459 - val_loss: 0.1956 - val_accuracy: 0.9475\n",
      "Epoch 774/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.1992 - accuracy: 0.9459 - val_loss: 0.1952 - val_accuracy: 0.9475\n",
      "Epoch 775/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2022 - accuracy: 0.9459 - val_loss: 0.1953 - val_accuracy: 0.9475\n",
      "Epoch 776/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2018 - accuracy: 0.9459 - val_loss: 0.1943 - val_accuracy: 0.9475\n",
      "Epoch 777/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2010 - accuracy: 0.9459 - val_loss: 0.1956 - val_accuracy: 0.9475\n",
      "Epoch 778/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2019 - accuracy: 0.9459 - val_loss: 0.1930 - val_accuracy: 0.9475\n",
      "Epoch 779/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2013 - accuracy: 0.9459 - val_loss: 0.1962 - val_accuracy: 0.9475\n",
      "Epoch 780/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2002 - accuracy: 0.9459 - val_loss: 0.1970 - val_accuracy: 0.9475\n",
      "Epoch 781/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2027 - accuracy: 0.9459 - val_loss: 0.1958 - val_accuracy: 0.9475\n",
      "Epoch 782/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2003 - accuracy: 0.9459 - val_loss: 0.1985 - val_accuracy: 0.9475\n",
      "Epoch 783/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2025 - accuracy: 0.9459 - val_loss: 0.1976 - val_accuracy: 0.9475\n",
      "Epoch 784/1000\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 0.2022 - accuracy: 0.9459 - val_loss: 0.1937 - val_accuracy: 0.9475\n",
      "Epoch 785/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2010 - accuracy: 0.9459 - val_loss: 0.1953 - val_accuracy: 0.9475\n",
      "Epoch 786/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2018 - accuracy: 0.9459 - val_loss: 0.1939 - val_accuracy: 0.9475\n",
      "Epoch 787/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2011 - accuracy: 0.9459 - val_loss: 0.1958 - val_accuracy: 0.9475\n",
      "Epoch 788/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2013 - accuracy: 0.9459 - val_loss: 0.1969 - val_accuracy: 0.9475\n",
      "Epoch 789/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2012 - accuracy: 0.9459 - val_loss: 0.1961 - val_accuracy: 0.9475\n",
      "Epoch 790/1000\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 0.2028 - accuracy: 0.9459 - val_loss: 0.1941 - val_accuracy: 0.9475\n",
      "Epoch 791/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2019 - accuracy: 0.9459 - val_loss: 0.1944 - val_accuracy: 0.9475\n",
      "Epoch 792/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2014 - accuracy: 0.9459 - val_loss: 0.1934 - val_accuracy: 0.9475\n",
      "Epoch 793/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2009 - accuracy: 0.9459 - val_loss: 0.1938 - val_accuracy: 0.9475\n",
      "Epoch 794/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2017 - accuracy: 0.9459 - val_loss: 0.1935 - val_accuracy: 0.9475\n",
      "Epoch 795/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2010 - accuracy: 0.9459 - val_loss: 0.1939 - val_accuracy: 0.9475\n",
      "Epoch 796/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.1989 - accuracy: 0.9459 - val_loss: 0.1941 - val_accuracy: 0.9475\n",
      "Epoch 797/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.1991 - accuracy: 0.9459 - val_loss: 0.1934 - val_accuracy: 0.9475\n",
      "Epoch 798/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2034 - accuracy: 0.9459 - val_loss: 0.1937 - val_accuracy: 0.9475\n",
      "Epoch 799/1000\n",
      "67/67 [==============================] - 13s 197ms/step - loss: 0.2029 - accuracy: 0.9459 - val_loss: 0.1947 - val_accuracy: 0.9475\n",
      "Epoch 800/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2016 - accuracy: 0.9459 - val_loss: 0.1934 - val_accuracy: 0.9475\n",
      "Epoch 801/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2016 - accuracy: 0.9459 - val_loss: 0.1945 - val_accuracy: 0.9475\n",
      "Epoch 802/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2006 - accuracy: 0.9459 - val_loss: 0.1936 - val_accuracy: 0.9475\n",
      "Epoch 803/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2004 - accuracy: 0.9459 - val_loss: 0.1951 - val_accuracy: 0.9475\n",
      "Epoch 804/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2016 - accuracy: 0.9459 - val_loss: 0.1973 - val_accuracy: 0.9475\n",
      "Epoch 805/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2002 - accuracy: 0.9459 - val_loss: 0.1983 - val_accuracy: 0.9475\n",
      "Epoch 806/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2019 - accuracy: 0.9459 - val_loss: 0.1986 - val_accuracy: 0.9475\n",
      "Epoch 807/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2016 - accuracy: 0.9459 - val_loss: 0.1942 - val_accuracy: 0.9475\n",
      "Epoch 808/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2016 - accuracy: 0.9459 - val_loss: 0.1943 - val_accuracy: 0.9475\n",
      "Epoch 809/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2007 - accuracy: 0.9459 - val_loss: 0.1927 - val_accuracy: 0.9475\n",
      "Epoch 810/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2004 - accuracy: 0.9459 - val_loss: 0.1953 - val_accuracy: 0.9475\n",
      "Epoch 811/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.1999 - accuracy: 0.9459 - val_loss: 0.1947 - val_accuracy: 0.9475\n",
      "Epoch 812/1000\n",
      "67/67 [==============================] - 13s 196ms/step - loss: 0.2008 - accuracy: 0.9459 - val_loss: 0.1944 - val_accuracy: 0.9475\n",
      "Epoch 813/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2015 - accuracy: 0.9459 - val_loss: 0.1941 - val_accuracy: 0.9475\n",
      "Epoch 814/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2010 - accuracy: 0.9459 - val_loss: 0.1948 - val_accuracy: 0.9475\n",
      "Epoch 815/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2014 - accuracy: 0.9459 - val_loss: 0.1941 - val_accuracy: 0.9475\n",
      "Epoch 816/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2020 - accuracy: 0.9459 - val_loss: 0.1935 - val_accuracy: 0.9475\n",
      "Epoch 817/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2008 - accuracy: 0.9459 - val_loss: 0.1930 - val_accuracy: 0.9475\n",
      "Epoch 818/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2028 - accuracy: 0.9459 - val_loss: 0.1931 - val_accuracy: 0.9475\n",
      "Epoch 819/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.1996 - accuracy: 0.9459 - val_loss: 0.1932 - val_accuracy: 0.9475\n",
      "Epoch 820/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.1989 - accuracy: 0.9459 - val_loss: 0.1994 - val_accuracy: 0.9475\n",
      "Epoch 821/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2014 - accuracy: 0.9459 - val_loss: 0.1934 - val_accuracy: 0.9475\n",
      "Epoch 822/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2014 - accuracy: 0.9459 - val_loss: 0.1943 - val_accuracy: 0.9475\n",
      "Epoch 823/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2008 - accuracy: 0.9459 - val_loss: 0.1937 - val_accuracy: 0.9475\n",
      "Epoch 824/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2012 - accuracy: 0.9459 - val_loss: 0.1935 - val_accuracy: 0.9475\n",
      "Epoch 825/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2024 - accuracy: 0.9459 - val_loss: 0.1942 - val_accuracy: 0.9475\n",
      "Epoch 826/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2007 - accuracy: 0.9459 - val_loss: 0.1976 - val_accuracy: 0.9475\n",
      "Epoch 827/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2012 - accuracy: 0.9459 - val_loss: 0.1970 - val_accuracy: 0.9475\n",
      "Epoch 828/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2017 - accuracy: 0.9459 - val_loss: 0.1942 - val_accuracy: 0.9475\n",
      "Epoch 829/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2010 - accuracy: 0.9459 - val_loss: 0.1938 - val_accuracy: 0.9475\n",
      "Epoch 830/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2012 - accuracy: 0.9459 - val_loss: 0.1983 - val_accuracy: 0.9475\n",
      "Epoch 831/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2035 - accuracy: 0.9459 - val_loss: 0.1970 - val_accuracy: 0.9475\n",
      "Epoch 832/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2025 - accuracy: 0.9459 - val_loss: 0.1937 - val_accuracy: 0.9475\n",
      "Epoch 833/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2006 - accuracy: 0.9459 - val_loss: 0.1974 - val_accuracy: 0.9475\n",
      "Epoch 834/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2010 - accuracy: 0.9459 - val_loss: 0.1947 - val_accuracy: 0.9475\n",
      "Epoch 835/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2021 - accuracy: 0.9459 - val_loss: 0.1985 - val_accuracy: 0.9475\n",
      "Epoch 836/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2032 - accuracy: 0.9459 - val_loss: 0.1960 - val_accuracy: 0.9475\n",
      "Epoch 837/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.1999 - accuracy: 0.9459 - val_loss: 0.1938 - val_accuracy: 0.9475\n",
      "Epoch 838/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2016 - accuracy: 0.9459 - val_loss: 0.1940 - val_accuracy: 0.9475\n",
      "Epoch 839/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2015 - accuracy: 0.9459 - val_loss: 0.1997 - val_accuracy: 0.9475\n",
      "Epoch 840/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2016 - accuracy: 0.9459 - val_loss: 0.1930 - val_accuracy: 0.9475\n",
      "Epoch 841/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2017 - accuracy: 0.9459 - val_loss: 0.1945 - val_accuracy: 0.9475\n",
      "Epoch 842/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.1996 - accuracy: 0.9459 - val_loss: 0.1957 - val_accuracy: 0.9475\n",
      "Epoch 843/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2016 - accuracy: 0.9459 - val_loss: 0.1935 - val_accuracy: 0.9475\n",
      "Epoch 844/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2015 - accuracy: 0.9459 - val_loss: 0.1949 - val_accuracy: 0.9475\n",
      "Epoch 845/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2002 - accuracy: 0.9459 - val_loss: 0.1947 - val_accuracy: 0.9475\n",
      "Epoch 846/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2002 - accuracy: 0.9459 - val_loss: 0.1963 - val_accuracy: 0.9475\n",
      "Epoch 847/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2015 - accuracy: 0.9459 - val_loss: 0.1933 - val_accuracy: 0.9475\n",
      "Epoch 848/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2026 - accuracy: 0.9459 - val_loss: 0.1954 - val_accuracy: 0.9475\n",
      "Epoch 849/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2007 - accuracy: 0.9459 - val_loss: 0.1942 - val_accuracy: 0.9475\n",
      "Epoch 850/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2011 - accuracy: 0.9459 - val_loss: 0.1940 - val_accuracy: 0.9475\n",
      "Epoch 851/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2007 - accuracy: 0.9459 - val_loss: 0.1938 - val_accuracy: 0.9475\n",
      "Epoch 852/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2024 - accuracy: 0.9459 - val_loss: 0.1959 - val_accuracy: 0.9475\n",
      "Epoch 853/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2002 - accuracy: 0.9459 - val_loss: 0.1930 - val_accuracy: 0.9475\n",
      "Epoch 854/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2005 - accuracy: 0.9459 - val_loss: 0.1944 - val_accuracy: 0.9475\n",
      "Epoch 855/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.1999 - accuracy: 0.9459 - val_loss: 0.1943 - val_accuracy: 0.9475\n",
      "Epoch 856/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2044 - accuracy: 0.9459 - val_loss: 0.1933 - val_accuracy: 0.9475\n",
      "Epoch 857/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2009 - accuracy: 0.9459 - val_loss: 0.1939 - val_accuracy: 0.9475\n",
      "Epoch 858/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2010 - accuracy: 0.9459 - val_loss: 0.1942 - val_accuracy: 0.9475\n",
      "Epoch 859/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2012 - accuracy: 0.9459 - val_loss: 0.1932 - val_accuracy: 0.9475\n",
      "Epoch 860/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2007 - accuracy: 0.9459 - val_loss: 0.1958 - val_accuracy: 0.9475\n",
      "Epoch 861/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2001 - accuracy: 0.9459 - val_loss: 0.1943 - val_accuracy: 0.9475\n",
      "Epoch 862/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2010 - accuracy: 0.9459 - val_loss: 0.1938 - val_accuracy: 0.9475\n",
      "Epoch 863/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2010 - accuracy: 0.9459 - val_loss: 0.1941 - val_accuracy: 0.9475\n",
      "Epoch 864/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.1994 - accuracy: 0.9459 - val_loss: 0.1942 - val_accuracy: 0.9475\n",
      "Epoch 865/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2022 - accuracy: 0.9459 - val_loss: 0.1942 - val_accuracy: 0.9475\n",
      "Epoch 866/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2003 - accuracy: 0.9459 - val_loss: 0.1946 - val_accuracy: 0.9475\n",
      "Epoch 867/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2010 - accuracy: 0.9459 - val_loss: 0.1998 - val_accuracy: 0.9475\n",
      "Epoch 868/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2009 - accuracy: 0.9459 - val_loss: 0.1950 - val_accuracy: 0.9475\n",
      "Epoch 869/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2001 - accuracy: 0.9459 - val_loss: 0.1944 - val_accuracy: 0.9475\n",
      "Epoch 870/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2002 - accuracy: 0.9459 - val_loss: 0.1945 - val_accuracy: 0.9475\n",
      "Epoch 871/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2030 - accuracy: 0.9459 - val_loss: 0.1949 - val_accuracy: 0.9475\n",
      "Epoch 872/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2005 - accuracy: 0.9459 - val_loss: 0.1944 - val_accuracy: 0.9475\n",
      "Epoch 873/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2011 - accuracy: 0.9459 - val_loss: 0.1930 - val_accuracy: 0.9475\n",
      "Epoch 874/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2008 - accuracy: 0.9459 - val_loss: 0.1932 - val_accuracy: 0.9475\n",
      "Epoch 875/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2006 - accuracy: 0.9459 - val_loss: 0.1928 - val_accuracy: 0.9475\n",
      "Epoch 876/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2006 - accuracy: 0.9459 - val_loss: 0.1940 - val_accuracy: 0.9475\n",
      "Epoch 877/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2003 - accuracy: 0.9459 - val_loss: 0.1944 - val_accuracy: 0.9475\n",
      "Epoch 878/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2016 - accuracy: 0.9459 - val_loss: 0.1950 - val_accuracy: 0.9475\n",
      "Epoch 879/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2008 - accuracy: 0.9459 - val_loss: 0.1971 - val_accuracy: 0.9475\n",
      "Epoch 880/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2031 - accuracy: 0.9459 - val_loss: 0.1935 - val_accuracy: 0.9475\n",
      "Epoch 881/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.1991 - accuracy: 0.9459 - val_loss: 0.1964 - val_accuracy: 0.9475\n",
      "Epoch 882/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2018 - accuracy: 0.9459 - val_loss: 0.1927 - val_accuracy: 0.9475\n",
      "Epoch 883/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2005 - accuracy: 0.9459 - val_loss: 0.1929 - val_accuracy: 0.9475\n",
      "Epoch 884/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2013 - accuracy: 0.9459 - val_loss: 0.1947 - val_accuracy: 0.9475\n",
      "Epoch 885/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2037 - accuracy: 0.9459 - val_loss: 0.2038 - val_accuracy: 0.9475\n",
      "Epoch 886/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2025 - accuracy: 0.9459 - val_loss: 0.2013 - val_accuracy: 0.9475\n",
      "Epoch 887/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2018 - accuracy: 0.9459 - val_loss: 0.1927 - val_accuracy: 0.9475\n",
      "Epoch 888/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2019 - accuracy: 0.9459 - val_loss: 0.1936 - val_accuracy: 0.9475\n",
      "Epoch 889/1000\n",
      "67/67 [==============================] - 13s 196ms/step - loss: 0.2007 - accuracy: 0.9459 - val_loss: 0.1934 - val_accuracy: 0.9475\n",
      "Epoch 890/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.1994 - accuracy: 0.9459 - val_loss: 0.1939 - val_accuracy: 0.9475\n",
      "Epoch 891/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2011 - accuracy: 0.9459 - val_loss: 0.1935 - val_accuracy: 0.9475\n",
      "Epoch 892/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2001 - accuracy: 0.9459 - val_loss: 0.1930 - val_accuracy: 0.9475\n",
      "Epoch 893/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.1998 - accuracy: 0.9459 - val_loss: 0.1949 - val_accuracy: 0.9475\n",
      "Epoch 894/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2016 - accuracy: 0.9459 - val_loss: 0.1944 - val_accuracy: 0.9475\n",
      "Epoch 895/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2036 - accuracy: 0.9459 - val_loss: 0.1951 - val_accuracy: 0.9475\n",
      "Epoch 896/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2007 - accuracy: 0.9459 - val_loss: 0.1927 - val_accuracy: 0.9475\n",
      "Epoch 897/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.1999 - accuracy: 0.9459 - val_loss: 0.1928 - val_accuracy: 0.9475\n",
      "Epoch 898/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2005 - accuracy: 0.9459 - val_loss: 0.1949 - val_accuracy: 0.9475\n",
      "Epoch 899/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2006 - accuracy: 0.9459 - val_loss: 0.1961 - val_accuracy: 0.9475\n",
      "Epoch 900/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2022 - accuracy: 0.9459 - val_loss: 0.1943 - val_accuracy: 0.9475\n",
      "Epoch 901/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.1997 - accuracy: 0.9459 - val_loss: 0.1974 - val_accuracy: 0.9475\n",
      "Epoch 902/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2007 - accuracy: 0.9459 - val_loss: 0.1939 - val_accuracy: 0.9475\n",
      "Epoch 903/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2016 - accuracy: 0.9459 - val_loss: 0.1949 - val_accuracy: 0.9475\n",
      "Epoch 904/1000\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 0.2008 - accuracy: 0.9459 - val_loss: 0.1935 - val_accuracy: 0.9475\n",
      "Epoch 905/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.1999 - accuracy: 0.9459 - val_loss: 0.1939 - val_accuracy: 0.9475\n",
      "Epoch 906/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2003 - accuracy: 0.9459 - val_loss: 0.1929 - val_accuracy: 0.9475\n",
      "Epoch 907/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2015 - accuracy: 0.9459 - val_loss: 0.1957 - val_accuracy: 0.9475\n",
      "Epoch 908/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2002 - accuracy: 0.9459 - val_loss: 0.1948 - val_accuracy: 0.9475\n",
      "Epoch 909/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2022 - accuracy: 0.9459 - val_loss: 0.1940 - val_accuracy: 0.9475\n",
      "Epoch 910/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2013 - accuracy: 0.9459 - val_loss: 0.1931 - val_accuracy: 0.9475\n",
      "Epoch 911/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2001 - accuracy: 0.9459 - val_loss: 0.1954 - val_accuracy: 0.9475\n",
      "Epoch 912/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2006 - accuracy: 0.9459 - val_loss: 0.1931 - val_accuracy: 0.9475\n",
      "Epoch 913/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2009 - accuracy: 0.9459 - val_loss: 0.1929 - val_accuracy: 0.9475\n",
      "Epoch 914/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2002 - accuracy: 0.9459 - val_loss: 0.1967 - val_accuracy: 0.9475\n",
      "Epoch 915/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2018 - accuracy: 0.9459 - val_loss: 0.1957 - val_accuracy: 0.9475\n",
      "Epoch 916/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2011 - accuracy: 0.9459 - val_loss: 0.1932 - val_accuracy: 0.9475\n",
      "Epoch 917/1000\n",
      "67/67 [==============================] - 13s 196ms/step - loss: 0.2018 - accuracy: 0.9459 - val_loss: 0.1952 - val_accuracy: 0.9475\n",
      "Epoch 918/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2030 - accuracy: 0.9459 - val_loss: 0.1947 - val_accuracy: 0.9475\n",
      "Epoch 919/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2005 - accuracy: 0.9459 - val_loss: 0.1957 - val_accuracy: 0.9475\n",
      "Epoch 920/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2025 - accuracy: 0.9459 - val_loss: 0.1930 - val_accuracy: 0.9475\n",
      "Epoch 921/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2021 - accuracy: 0.9459 - val_loss: 0.1939 - val_accuracy: 0.9475\n",
      "Epoch 922/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.1994 - accuracy: 0.9459 - val_loss: 0.1933 - val_accuracy: 0.9475\n",
      "Epoch 923/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2001 - accuracy: 0.9459 - val_loss: 0.1951 - val_accuracy: 0.9475\n",
      "Epoch 924/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2011 - accuracy: 0.9459 - val_loss: 0.1943 - val_accuracy: 0.9475\n",
      "Epoch 925/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2002 - accuracy: 0.9459 - val_loss: 0.1933 - val_accuracy: 0.9475\n",
      "Epoch 926/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2006 - accuracy: 0.9459 - val_loss: 0.1957 - val_accuracy: 0.9475\n",
      "Epoch 927/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.1986 - accuracy: 0.9459 - val_loss: 0.1938 - val_accuracy: 0.9475\n",
      "Epoch 928/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2010 - accuracy: 0.9459 - val_loss: 0.1943 - val_accuracy: 0.9475\n",
      "Epoch 929/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2001 - accuracy: 0.9459 - val_loss: 0.1938 - val_accuracy: 0.9475\n",
      "Epoch 930/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2041 - accuracy: 0.9459 - val_loss: 0.1943 - val_accuracy: 0.9475\n",
      "Epoch 931/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2009 - accuracy: 0.9459 - val_loss: 0.1933 - val_accuracy: 0.9475\n",
      "Epoch 932/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2009 - accuracy: 0.9459 - val_loss: 0.1934 - val_accuracy: 0.9475\n",
      "Epoch 933/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2020 - accuracy: 0.9459 - val_loss: 0.1962 - val_accuracy: 0.9475\n",
      "Epoch 934/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2022 - accuracy: 0.9459 - val_loss: 0.1974 - val_accuracy: 0.9475\n",
      "Epoch 935/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2014 - accuracy: 0.9459 - val_loss: 0.1928 - val_accuracy: 0.9475\n",
      "Epoch 936/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2010 - accuracy: 0.9459 - val_loss: 0.1970 - val_accuracy: 0.9475\n",
      "Epoch 937/1000\n",
      "67/67 [==============================] - 13s 197ms/step - loss: 0.2005 - accuracy: 0.9459 - val_loss: 0.1932 - val_accuracy: 0.9475\n",
      "Epoch 938/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2000 - accuracy: 0.9459 - val_loss: 0.1927 - val_accuracy: 0.9475\n",
      "Epoch 939/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2003 - accuracy: 0.9459 - val_loss: 0.1934 - val_accuracy: 0.9475\n",
      "Epoch 940/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2026 - accuracy: 0.9459 - val_loss: 0.1936 - val_accuracy: 0.9475\n",
      "Epoch 941/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2006 - accuracy: 0.9459 - val_loss: 0.1982 - val_accuracy: 0.9475\n",
      "Epoch 942/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2001 - accuracy: 0.9459 - val_loss: 0.1942 - val_accuracy: 0.9475\n",
      "Epoch 943/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2024 - accuracy: 0.9459 - val_loss: 0.1964 - val_accuracy: 0.9475\n",
      "Epoch 944/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2023 - accuracy: 0.9459 - val_loss: 0.1937 - val_accuracy: 0.9475\n",
      "Epoch 945/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2024 - accuracy: 0.9459 - val_loss: 0.1940 - val_accuracy: 0.9475\n",
      "Epoch 946/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2000 - accuracy: 0.9459 - val_loss: 0.1928 - val_accuracy: 0.9475\n",
      "Epoch 947/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.1986 - accuracy: 0.9459 - val_loss: 0.1931 - val_accuracy: 0.9475\n",
      "Epoch 948/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2004 - accuracy: 0.9459 - val_loss: 0.1934 - val_accuracy: 0.9475\n",
      "Epoch 949/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2023 - accuracy: 0.9459 - val_loss: 0.1929 - val_accuracy: 0.9475\n",
      "Epoch 950/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2005 - accuracy: 0.9459 - val_loss: 0.1942 - val_accuracy: 0.9475\n",
      "Epoch 951/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2008 - accuracy: 0.9459 - val_loss: 0.1937 - val_accuracy: 0.9475\n",
      "Epoch 952/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.1998 - accuracy: 0.9459 - val_loss: 0.1934 - val_accuracy: 0.9475\n",
      "Epoch 953/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.1995 - accuracy: 0.9459 - val_loss: 0.1926 - val_accuracy: 0.9475\n",
      "Epoch 954/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2016 - accuracy: 0.9459 - val_loss: 0.1945 - val_accuracy: 0.9475\n",
      "Epoch 955/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2014 - accuracy: 0.9459 - val_loss: 0.1933 - val_accuracy: 0.9475\n",
      "Epoch 956/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.1996 - accuracy: 0.9459 - val_loss: 0.1932 - val_accuracy: 0.9475\n",
      "Epoch 957/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2005 - accuracy: 0.9459 - val_loss: 0.1928 - val_accuracy: 0.9475\n",
      "Epoch 958/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2001 - accuracy: 0.9459 - val_loss: 0.1953 - val_accuracy: 0.9475\n",
      "Epoch 959/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2023 - accuracy: 0.9459 - val_loss: 0.1933 - val_accuracy: 0.9475\n",
      "Epoch 960/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2012 - accuracy: 0.9459 - val_loss: 0.1924 - val_accuracy: 0.9475\n",
      "Epoch 961/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2004 - accuracy: 0.9459 - val_loss: 0.1954 - val_accuracy: 0.9475\n",
      "Epoch 962/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2030 - accuracy: 0.9459 - val_loss: 0.1940 - val_accuracy: 0.9475\n",
      "Epoch 963/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2005 - accuracy: 0.9459 - val_loss: 0.1931 - val_accuracy: 0.9475\n",
      "Epoch 964/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.1991 - accuracy: 0.9459 - val_loss: 0.1935 - val_accuracy: 0.9475\n",
      "Epoch 965/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2019 - accuracy: 0.9459 - val_loss: 0.1937 - val_accuracy: 0.9475\n",
      "Epoch 966/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2003 - accuracy: 0.9459 - val_loss: 0.1947 - val_accuracy: 0.9475\n",
      "Epoch 967/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2004 - accuracy: 0.9459 - val_loss: 0.1945 - val_accuracy: 0.9475\n",
      "Epoch 968/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2007 - accuracy: 0.9459 - val_loss: 0.1963 - val_accuracy: 0.9475\n",
      "Epoch 969/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.1998 - accuracy: 0.9459 - val_loss: 0.1973 - val_accuracy: 0.9475\n",
      "Epoch 970/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2007 - accuracy: 0.9459 - val_loss: 0.1969 - val_accuracy: 0.9475\n",
      "Epoch 971/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2000 - accuracy: 0.9459 - val_loss: 0.1951 - val_accuracy: 0.9475\n",
      "Epoch 972/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.1998 - accuracy: 0.9459 - val_loss: 0.1938 - val_accuracy: 0.9475\n",
      "Epoch 973/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2010 - accuracy: 0.9459 - val_loss: 0.1935 - val_accuracy: 0.9475\n",
      "Epoch 974/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2013 - accuracy: 0.9459 - val_loss: 0.1927 - val_accuracy: 0.9475\n",
      "Epoch 975/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.1993 - accuracy: 0.9459 - val_loss: 0.1990 - val_accuracy: 0.9475\n",
      "Epoch 976/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.1999 - accuracy: 0.9459 - val_loss: 0.1930 - val_accuracy: 0.9475\n",
      "Epoch 977/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2020 - accuracy: 0.9459 - val_loss: 0.1976 - val_accuracy: 0.9475\n",
      "Epoch 978/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2009 - accuracy: 0.9459 - val_loss: 0.1945 - val_accuracy: 0.9475\n",
      "Epoch 979/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2009 - accuracy: 0.9459 - val_loss: 0.1931 - val_accuracy: 0.9475\n",
      "Epoch 980/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2004 - accuracy: 0.9459 - val_loss: 0.1942 - val_accuracy: 0.9475\n",
      "Epoch 981/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2004 - accuracy: 0.9459 - val_loss: 0.1929 - val_accuracy: 0.9475\n",
      "Epoch 982/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2005 - accuracy: 0.9459 - val_loss: 0.1970 - val_accuracy: 0.9475\n",
      "Epoch 983/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2023 - accuracy: 0.9459 - val_loss: 0.1924 - val_accuracy: 0.9475\n",
      "Epoch 984/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.1998 - accuracy: 0.9459 - val_loss: 0.1939 - val_accuracy: 0.9475\n",
      "Epoch 985/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2006 - accuracy: 0.9459 - val_loss: 0.1937 - val_accuracy: 0.9475\n",
      "Epoch 986/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2018 - accuracy: 0.9459 - val_loss: 0.1928 - val_accuracy: 0.9475\n",
      "Epoch 987/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.1999 - accuracy: 0.9459 - val_loss: 0.1927 - val_accuracy: 0.9475\n",
      "Epoch 988/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2009 - accuracy: 0.9459 - val_loss: 0.1928 - val_accuracy: 0.9475\n",
      "Epoch 989/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.1996 - accuracy: 0.9459 - val_loss: 0.1938 - val_accuracy: 0.9475\n",
      "Epoch 990/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2016 - accuracy: 0.9459 - val_loss: 0.1954 - val_accuracy: 0.9475\n",
      "Epoch 991/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2033 - accuracy: 0.9459 - val_loss: 0.1940 - val_accuracy: 0.9475\n",
      "Epoch 992/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.1999 - accuracy: 0.9459 - val_loss: 0.1960 - val_accuracy: 0.9475\n",
      "Epoch 993/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2004 - accuracy: 0.9459 - val_loss: 0.1929 - val_accuracy: 0.9475\n",
      "Epoch 994/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2012 - accuracy: 0.9459 - val_loss: 0.1925 - val_accuracy: 0.9475\n",
      "Epoch 995/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2015 - accuracy: 0.9459 - val_loss: 0.1973 - val_accuracy: 0.9475\n",
      "Epoch 996/1000\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.2013 - accuracy: 0.9459 - val_loss: 0.1928 - val_accuracy: 0.9475\n",
      "Epoch 997/1000\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2003 - accuracy: 0.9459 - val_loss: 0.1936 - val_accuracy: 0.9475\n",
      "Epoch 998/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2025 - accuracy: 0.9459 - val_loss: 0.1933 - val_accuracy: 0.9475\n",
      "Epoch 999/1000\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.2031 - accuracy: 0.9459 - val_loss: 0.1947 - val_accuracy: 0.9475\n",
      "Epoch 1000/1000\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 0.2022 - accuracy: 0.9459 - val_loss: 0.1929 - val_accuracy: 0.9475\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)\n",
    "history=model.fit([X_img_train, X_audio_train], y_train, batch_size=256, epochs=1000,\n",
    "                            validation_data=([X_img_test, X_audio_test], y_test), callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "df=pd.DataFrame(history.history)\n",
    "df.to_csv('fakeavceleb_MAV_net.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(hist):\n",
    "    plt.plot(hist.history[\"accuracy\"])\n",
    "    plt.plot(hist.history[\"val_accuracy\"])\n",
    "    plt.title(\"model accuracy\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"lower right\")\n",
    "    plt.savefig('fakeav_combined_acc.eps', format='eps')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(hist):\n",
    "    plt.plot(hist.history['loss'])\n",
    "    plt.plot(hist.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper right')\n",
    "    plt.savefig('fakeav_combined_loss.eps', format='eps')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"388.965625pt\" height=\"277.314375pt\" viewBox=\"0 0 388.965625 277.314375\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2022-07-06T00:48:50.080470</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.5.1, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M -0 277.314375 \n",
       "L 388.965625 277.314375 \n",
       "L 388.965625 0 \n",
       "L -0 0 \n",
       "L -0 277.314375 \n",
       "z\n",
       "\" style=\"fill: none\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 46.965625 239.758125 \n",
       "L 381.765625 239.758125 \n",
       "L 381.765625 22.318125 \n",
       "L 46.965625 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"mb8b0de1c56\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb8b0de1c56\" x=\"62.183807\" y=\"239.758125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(59.002557 254.356562)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb8b0de1c56\" x=\"123.117468\" y=\"239.758125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 200 -->\n",
       "      <g transform=\"translate(113.573718 254.356562)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb8b0de1c56\" x=\"184.051129\" y=\"239.758125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 400 -->\n",
       "      <g transform=\"translate(174.507379 254.356562)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb8b0de1c56\" x=\"244.98479\" y=\"239.758125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 600 -->\n",
       "      <g transform=\"translate(235.44104 254.356562)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb8b0de1c56\" x=\"305.918451\" y=\"239.758125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 800 -->\n",
       "      <g transform=\"translate(296.374701 254.356562)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb8b0de1c56\" x=\"366.852111\" y=\"239.758125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 1000 -->\n",
       "      <g transform=\"translate(354.127111 254.356562)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_7\">\n",
       "     <!-- Epoch -->\n",
       "     <g transform=\"translate(199.054688 268.034687)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-45\" d=\"M 628 4666 \n",
       "L 3578 4666 \n",
       "L 3578 4134 \n",
       "L 1259 4134 \n",
       "L 1259 2753 \n",
       "L 3481 2753 \n",
       "L 3481 2222 \n",
       "L 1259 2222 \n",
       "L 1259 531 \n",
       "L 3634 531 \n",
       "L 3634 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-45\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"63.183594\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"126.660156\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"187.841797\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-68\" x=\"242.822266\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <defs>\n",
       "       <path id=\"m95c1848c5f\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m95c1848c5f\" x=\"46.965625\" y=\"230.06115\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(33.603125 233.860369)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m95c1848c5f\" x=\"46.965625\" y=\"205.802252\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 25 -->\n",
       "      <g transform=\"translate(27.240625 209.60147)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m95c1848c5f\" x=\"46.965625\" y=\"181.543353\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 50 -->\n",
       "      <g transform=\"translate(27.240625 185.342572)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m95c1848c5f\" x=\"46.965625\" y=\"157.284455\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 75 -->\n",
       "      <g transform=\"translate(27.240625 161.083674)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \n",
       "L 3525 4666 \n",
       "L 3525 4397 \n",
       "L 1831 0 \n",
       "L 1172 0 \n",
       "L 2766 4134 \n",
       "L 525 4134 \n",
       "L 525 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-37\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m95c1848c5f\" x=\"46.965625\" y=\"133.025557\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 100 -->\n",
       "      <g transform=\"translate(20.878125 136.824776)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m95c1848c5f\" x=\"46.965625\" y=\"108.766658\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 125 -->\n",
       "      <g transform=\"translate(20.878125 112.565877)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m95c1848c5f\" x=\"46.965625\" y=\"84.50776\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 150 -->\n",
       "      <g transform=\"translate(20.878125 88.306979)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_8\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m95c1848c5f\" x=\"46.965625\" y=\"60.248862\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 175 -->\n",
       "      <g transform=\"translate(20.878125 64.048081)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-37\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_9\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m95c1848c5f\" x=\"46.965625\" y=\"35.989964\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_16\">\n",
       "      <!-- 200 -->\n",
       "      <g transform=\"translate(20.878125 39.789182)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_17\">\n",
       "     <!-- Loss -->\n",
       "     <g transform=\"translate(14.798438 142.005312)rotate(-90)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-4c\" d=\"M 628 4666 \n",
       "L 1259 4666 \n",
       "L 1259 531 \n",
       "L 3531 531 \n",
       "L 3531 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-4c\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"53.962891\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"115.144531\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"167.244141\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_16\">\n",
       "    <path d=\"M 62.183807 32.201761 \n",
       "L 62.488475 176.354283 \n",
       "L 62.793143 191.968842 \n",
       "L 63.097812 199.786153 \n",
       "L 63.707148 207.642724 \n",
       "L 64.316485 212.056406 \n",
       "L 64.925822 215.047507 \n",
       "L 65.839826 217.902844 \n",
       "L 66.753831 219.722477 \n",
       "L 67.667836 220.944221 \n",
       "L 69.800514 222.632727 \n",
       "L 73.151866 224.290897 \n",
       "L 73.761202 224.474707 \n",
       "L 74.370539 224.71117 \n",
       "L 76.503217 225.427262 \n",
       "L 77.72189 225.752625 \n",
       "L 80.159237 226.442663 \n",
       "L 91.127296 228.726972 \n",
       "L 99.048672 229.542631 \n",
       "L 103.923365 229.750241 \n",
       "L 112.149409 229.849672 \n",
       "L 270.576927 229.864142 \n",
       "L 292.817713 229.865957 \n",
       "L 366.547443 229.864923 \n",
       "L 366.547443 229.864923 \n",
       "\" clip-path=\"url(#p5e50f84da5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_17\">\n",
       "    <path d=\"M 62.183807 177.692512 \n",
       "L 62.488475 193.106082 \n",
       "L 63.097812 203.34912 \n",
       "L 63.707148 209.14083 \n",
       "L 64.316485 212.991934 \n",
       "L 64.925822 215.699746 \n",
       "L 65.839826 218.305595 \n",
       "L 66.753831 219.989947 \n",
       "L 67.667836 221.138116 \n",
       "L 69.495846 222.58081 \n",
       "L 69.800514 222.626858 \n",
       "L 70.409851 223.126369 \n",
       "L 71.323856 223.546383 \n",
       "L 71.933193 223.850905 \n",
       "L 72.237861 223.906792 \n",
       "L 73.151866 224.367749 \n",
       "L 73.456534 224.405522 \n",
       "L 74.370539 224.806598 \n",
       "L 80.463905 226.549268 \n",
       "L 80.768573 226.471727 \n",
       "L 81.37791 226.800698 \n",
       "L 82.901252 227.116416 \n",
       "L 84.424593 227.509565 \n",
       "L 85.338598 227.650996 \n",
       "L 86.861939 227.990667 \n",
       "L 89.299286 228.471121 \n",
       "L 96.611325 229.382168 \n",
       "L 104.837369 229.779989 \n",
       "L 110.321399 229.845876 \n",
       "L 121.594126 229.865758 \n",
       "L 162.724347 229.869161 \n",
       "L 191.972505 229.869983 \n",
       "L 226.095355 229.870712 \n",
       "L 228.837369 229.870258 \n",
       "L 314.449163 229.873114 \n",
       "L 322.065871 229.872489 \n",
       "L 366.547443 229.873938 \n",
       "L 366.547443 229.873938 \n",
       "\" clip-path=\"url(#p5e50f84da5)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 46.965625 239.758125 \n",
       "L 46.965625 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 381.765625 239.758125 \n",
       "L 381.765625 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 46.965625 239.758125 \n",
       "L 381.765625 239.758125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 46.965625 22.318125 \n",
       "L 381.765625 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_18\">\n",
       "    <!-- model loss -->\n",
       "    <g transform=\"translate(182.185938 16.318125)scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \n",
       "Q 3544 3216 3844 3400 \n",
       "Q 4144 3584 4550 3584 \n",
       "Q 5097 3584 5394 3201 \n",
       "Q 5691 2819 5691 2113 \n",
       "L 5691 0 \n",
       "L 5113 0 \n",
       "L 5113 2094 \n",
       "Q 5113 2597 4934 2840 \n",
       "Q 4756 3084 4391 3084 \n",
       "Q 3944 3084 3684 2787 \n",
       "Q 3425 2491 3425 1978 \n",
       "L 3425 0 \n",
       "L 2847 0 \n",
       "L 2847 2094 \n",
       "Q 2847 2600 2669 2842 \n",
       "Q 2491 3084 2119 3084 \n",
       "Q 1678 3084 1418 2786 \n",
       "Q 1159 2488 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1356 3278 1631 3431 \n",
       "Q 1906 3584 2284 3584 \n",
       "Q 2666 3584 2933 3390 \n",
       "Q 3200 3197 3328 2828 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \n",
       "L 2906 4863 \n",
       "L 3481 4863 \n",
       "L 3481 0 \n",
       "L 2906 0 \n",
       "L 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "z\n",
       "M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-6d\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"97.412109\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"158.59375\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"222.070312\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6c\" x=\"283.59375\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"311.376953\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6c\" x=\"343.164062\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"370.947266\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"432.128906\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" x=\"484.228516\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 293.53125 59.674375 \n",
       "L 374.765625 59.674375 \n",
       "Q 376.765625 59.674375 376.765625 57.674375 \n",
       "L 376.765625 29.318125 \n",
       "Q 376.765625 27.318125 374.765625 27.318125 \n",
       "L 293.53125 27.318125 \n",
       "Q 291.53125 27.318125 291.53125 29.318125 \n",
       "L 291.53125 57.674375 \n",
       "Q 291.53125 59.674375 293.53125 59.674375 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_18\">\n",
       "     <path d=\"M 295.53125 35.416562 \n",
       "L 305.53125 35.416562 \n",
       "L 315.53125 35.416562 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_19\">\n",
       "     <!-- train -->\n",
       "     <g transform=\"translate(323.53125 38.916562)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_19\">\n",
       "     <path d=\"M 295.53125 50.094687 \n",
       "L 305.53125 50.094687 \n",
       "L 315.53125 50.094687 \n",
       "\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_20\">\n",
       "     <!-- validation -->\n",
       "     <g transform=\"translate(323.53125 53.594687)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"148.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" x=\"176.025391\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"239.501953\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"300.78125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"339.990234\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"367.773438\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"428.955078\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p5e50f84da5\">\n",
       "   <rect x=\"46.965625\" y=\"22.318125\" width=\"334.8\" height=\"217.44\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"385.78125pt\" height=\"277.314375pt\" viewBox=\"0 0 385.78125 277.314375\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2022-07-06T00:48:50.233758</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.5.1, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 277.314375 \n",
       "L 385.78125 277.314375 \n",
       "L 385.78125 0 \n",
       "L 0 0 \n",
       "L 0 277.314375 \n",
       "z\n",
       "\" style=\"fill: none\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 43.78125 239.758125 \n",
       "L 378.58125 239.758125 \n",
       "L 378.58125 22.318125 \n",
       "L 43.78125 22.318125 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m5eeac45685\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5eeac45685\" x=\"58.999432\" y=\"239.758125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(55.818182 254.356562)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5eeac45685\" x=\"119.933093\" y=\"239.758125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 200 -->\n",
       "      <g transform=\"translate(110.389343 254.356562)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5eeac45685\" x=\"180.866754\" y=\"239.758125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 400 -->\n",
       "      <g transform=\"translate(171.323004 254.356562)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5eeac45685\" x=\"241.800415\" y=\"239.758125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 600 -->\n",
       "      <g transform=\"translate(232.256665 254.356562)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5eeac45685\" x=\"302.734076\" y=\"239.758125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 800 -->\n",
       "      <g transform=\"translate(293.190326 254.356562)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m5eeac45685\" x=\"363.667736\" y=\"239.758125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 1000 -->\n",
       "      <g transform=\"translate(350.942736 254.356562)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_7\">\n",
       "     <!-- epoch -->\n",
       "     <g transform=\"translate(195.953125 268.034687)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <defs>\n",
       "       <path id=\"m58a5bdb570\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m58a5bdb570\" x=\"43.78125\" y=\"208.351468\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.4 -->\n",
       "      <g transform=\"translate(20.878125 212.150687)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m58a5bdb570\" x=\"43.78125\" y=\"176.18046\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(20.878125 179.979679)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m58a5bdb570\" x=\"43.78125\" y=\"144.009452\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.6 -->\n",
       "      <g transform=\"translate(20.878125 147.808671)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m58a5bdb570\" x=\"43.78125\" y=\"111.838445\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.7 -->\n",
       "      <g transform=\"translate(20.878125 115.637663)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \n",
       "L 3525 4666 \n",
       "L 3525 4397 \n",
       "L 1831 0 \n",
       "L 1172 0 \n",
       "L 2766 4134 \n",
       "L 525 4134 \n",
       "L 525 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-37\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m58a5bdb570\" x=\"43.78125\" y=\"79.667437\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.8 -->\n",
       "      <g transform=\"translate(20.878125 83.466656)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m58a5bdb570\" x=\"43.78125\" y=\"47.496429\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.9 -->\n",
       "      <g transform=\"translate(20.878125 51.295648)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-39\" d=\"M 703 97 \n",
       "L 703 672 \n",
       "Q 941 559 1184 500 \n",
       "Q 1428 441 1663 441 \n",
       "Q 2288 441 2617 861 \n",
       "Q 2947 1281 2994 2138 \n",
       "Q 2813 1869 2534 1725 \n",
       "Q 2256 1581 1919 1581 \n",
       "Q 1219 1581 811 2004 \n",
       "Q 403 2428 403 3163 \n",
       "Q 403 3881 828 4315 \n",
       "Q 1253 4750 1959 4750 \n",
       "Q 2769 4750 3195 4129 \n",
       "Q 3622 3509 3622 2328 \n",
       "Q 3622 1225 3098 567 \n",
       "Q 2575 -91 1691 -91 \n",
       "Q 1453 -91 1209 -44 \n",
       "Q 966 3 703 97 \n",
       "z\n",
       "M 1959 2075 \n",
       "Q 2384 2075 2632 2365 \n",
       "Q 2881 2656 2881 3163 \n",
       "Q 2881 3666 2632 3958 \n",
       "Q 2384 4250 1959 4250 \n",
       "Q 1534 4250 1286 3958 \n",
       "Q 1038 3666 1038 3163 \n",
       "Q 1038 2656 1286 2365 \n",
       "Q 1534 2075 1959 2075 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-39\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_14\">\n",
       "     <!-- accuracy -->\n",
       "     <g transform=\"translate(14.798438 153.5975)rotate(-90)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n",
       "Q 1816 -950 1584 -1140 \n",
       "Q 1353 -1331 966 -1331 \n",
       "L 506 -1331 \n",
       "L 506 -850 \n",
       "L 844 -850 \n",
       "Q 1081 -850 1212 -737 \n",
       "Q 1344 -625 1503 -206 \n",
       "L 1606 56 \n",
       "L 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 763 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2059 -325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-61\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"61.279297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"116.259766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"171.240234\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"234.619141\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"275.732422\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"337.011719\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-79\" x=\"391.992188\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_13\">\n",
       "    <path d=\"M 58.999432 51.99821 \n",
       "L 59.3041 50.257294 \n",
       "L 59.608768 50.143756 \n",
       "L 59.913437 52.509139 \n",
       "L 60.218105 48.364987 \n",
       "L 60.522773 37.276113 \n",
       "L 60.827442 34.607969 \n",
       "L 61.13211 34.210578 \n",
       "L 61.436778 34.078114 \n",
       "L 61.741447 33.491519 \n",
       "L 62.046115 33.453667 \n",
       "L 62.350783 34.040281 \n",
       "L 62.655451 33.207665 \n",
       "L 62.96012 33.548278 \n",
       "L 63.264788 32.961664 \n",
       "L 63.569456 32.923812 \n",
       "L 63.874125 33.245518 \n",
       "L 64.178793 32.885979 \n",
       "L 64.483461 32.791367 \n",
       "L 64.78813 33.321203 \n",
       "L 65.092798 33.245518 \n",
       "L 65.702135 32.791367 \n",
       "L 66.006803 33.58613 \n",
       "L 66.311471 32.98059 \n",
       "L 66.616139 33.226591 \n",
       "L 66.920808 32.923812 \n",
       "L 67.225476 33.018442 \n",
       "L 67.834813 33.529352 \n",
       "L 68.139481 32.867052 \n",
       "L 68.444149 32.734589 \n",
       "L 68.748818 32.8292 \n",
       "L 69.053486 33.453667 \n",
       "L 69.358154 33.623964 \n",
       "L 69.662822 33.056275 \n",
       "L 69.967491 32.753515 \n",
       "L 70.272159 33.302277 \n",
       "L 70.576827 32.885979 \n",
       "L 70.881496 33.377981 \n",
       "L 71.186164 32.885979 \n",
       "L 71.490832 32.961664 \n",
       "L 71.795501 32.810274 \n",
       "L 72.100169 33.207665 \n",
       "L 72.404837 32.848126 \n",
       "L 72.709506 33.075202 \n",
       "L 73.014174 32.810274 \n",
       "L 73.318842 32.848126 \n",
       "L 73.62351 33.113054 \n",
       "L 73.928179 32.885979 \n",
       "L 74.232847 33.226591 \n",
       "L 75.146852 32.791367 \n",
       "L 75.756189 32.848126 \n",
       "L 76.060857 33.245518 \n",
       "L 76.365525 32.961664 \n",
       "L 76.670193 33.321203 \n",
       "L 76.974862 32.942738 \n",
       "L 77.27953 32.867052 \n",
       "L 77.584198 33.661816 \n",
       "L 78.498203 32.999516 \n",
       "L 79.10754 32.8292 \n",
       "L 79.716877 32.942738 \n",
       "L 80.326213 33.264425 \n",
       "L 80.630881 32.923812 \n",
       "L 80.93555 32.791367 \n",
       "L 81.544886 32.8292 \n",
       "L 81.849555 32.848126 \n",
       "L 82.154223 33.13198 \n",
       "L 82.458891 33.018442 \n",
       "L 82.76356 33.359055 \n",
       "L 83.068228 32.810274 \n",
       "L 83.372896 32.98059 \n",
       "L 83.982233 32.734589 \n",
       "L 84.286901 32.734589 \n",
       "L 84.591569 32.867052 \n",
       "L 85.200906 32.734589 \n",
       "L 87.028916 32.772441 \n",
       "L 89.161594 32.734589 \n",
       "L 363.363068 32.734589 \n",
       "L 363.363068 32.734589 \n",
       "\" clip-path=\"url(#p72e3d2c6e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_14\">\n",
       "    <path d=\"M 58.999432 32.201761 \n",
       "L 59.3041 229.874489 \n",
       "L 59.608768 72.462827 \n",
       "L 59.913437 32.201761 \n",
       "L 363.363068 32.201761 \n",
       "L 363.363068 32.201761 \n",
       "\" clip-path=\"url(#p72e3d2c6e4)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 43.78125 239.758125 \n",
       "L 43.78125 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 378.58125 239.758125 \n",
       "L 378.58125 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 43.78125 239.758125 \n",
       "L 378.58125 239.758125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 43.78125 22.318125 \n",
       "L 378.58125 22.318125 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_15\">\n",
       "    <!-- model accuracy -->\n",
       "    <g transform=\"translate(163.519688 16.318125)scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \n",
       "Q 3544 3216 3844 3400 \n",
       "Q 4144 3584 4550 3584 \n",
       "Q 5097 3584 5394 3201 \n",
       "Q 5691 2819 5691 2113 \n",
       "L 5691 0 \n",
       "L 5113 0 \n",
       "L 5113 2094 \n",
       "Q 5113 2597 4934 2840 \n",
       "Q 4756 3084 4391 3084 \n",
       "Q 3944 3084 3684 2787 \n",
       "Q 3425 2491 3425 1978 \n",
       "L 3425 0 \n",
       "L 2847 0 \n",
       "L 2847 2094 \n",
       "Q 2847 2600 2669 2842 \n",
       "Q 2491 3084 2119 3084 \n",
       "Q 1678 3084 1418 2786 \n",
       "Q 1159 2488 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1356 3278 1631 3431 \n",
       "Q 1906 3584 2284 3584 \n",
       "Q 2666 3584 2933 3390 \n",
       "Q 3200 3197 3328 2828 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \n",
       "L 2906 4863 \n",
       "L 3481 4863 \n",
       "L 3481 0 \n",
       "L 2906 0 \n",
       "L 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "z\n",
       "M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-6d\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" x=\"97.412109\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" x=\"158.59375\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" x=\"222.070312\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6c\" x=\"283.59375\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" x=\"311.376953\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"343.164062\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"404.443359\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"459.423828\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" x=\"514.404297\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" x=\"577.783203\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" x=\"618.896484\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" x=\"680.175781\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-79\" x=\"735.15625\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 290.346875 234.758125 \n",
       "L 371.58125 234.758125 \n",
       "Q 373.58125 234.758125 373.58125 232.758125 \n",
       "L 373.58125 204.401875 \n",
       "Q 373.58125 202.401875 371.58125 202.401875 \n",
       "L 290.346875 202.401875 \n",
       "Q 288.346875 202.401875 288.346875 204.401875 \n",
       "L 288.346875 232.758125 \n",
       "Q 288.346875 234.758125 290.346875 234.758125 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_15\">\n",
       "     <path d=\"M 292.346875 210.500312 \n",
       "L 302.346875 210.500312 \n",
       "L 312.346875 210.500312 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_16\">\n",
       "     <!-- train -->\n",
       "     <g transform=\"translate(320.346875 214.000312)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_16\">\n",
       "     <path d=\"M 292.346875 225.178437 \n",
       "L 302.346875 225.178437 \n",
       "L 312.346875 225.178437 \n",
       "\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_17\">\n",
       "     <!-- validation -->\n",
       "     <g transform=\"translate(320.346875 228.678437)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"148.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" x=\"176.025391\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"239.501953\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"300.78125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"339.990234\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"367.773438\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"428.955078\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p72e3d2c6e4\">\n",
       "   <rect x=\"43.78125\" y=\"22.318125\" width=\"334.8\" height=\"217.44\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_hist(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Loss: 0.195, Train_Acc: 0.946\n",
      "Test_Loss: 0.193, Test_Acc: 0.948\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "train_loss, train_acc = model.evaluate([X_img_train, X_audio_train], y_train, verbose=0)\n",
    "test_loss, test_acc = model.evaluate([X_img_test, X_audio_test], y_test, verbose=0)\n",
    "print('Train_Loss: %.3f, Train_Acc: %.3f' % (train_loss, train_acc))\n",
    "print('Test_Loss: %.3f, Test_Acc: %.3f' % (test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 2s 16ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict([X_img_test, X_audio_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 2s 15ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = (model.predict([X_img_test, X_audio_test]) > 0.5)*1    #.astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.9475417548812044\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1_score: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test,y_pred)\n",
    "recall = recall_score(y_test,y_pred)\n",
    "f1 = f1_score(y_test,y_pred)\n",
    "print(\"score:\", score)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1_score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[4028    0]\n",
      " [ 223    0]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "matrix = confusion_matrix(y_test, y_pred, labels=[0,1])\n",
    "print('Confusion matrix : \\n',matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      4028\n",
      "           1       0.00      0.00      0.00       223\n",
      "\n",
      "    accuracy                           0.95      4251\n",
      "   macro avg       0.47      0.50      0.49      4251\n",
      "weighted avg       0.90      0.95      0.92      4251\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report for precision, recall f1-score and accuracy\n",
    "matrix = classification_report(y_test, y_pred, labels=[0,1])\n",
    "print('Classification report : \\n',matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outcome values : \n",
      " 4028 0 223 0\n"
     ]
    }
   ],
   "source": [
    "# outcome values order in sklearn\n",
    "tp, fn, fp, tn = confusion_matrix(y_test, y_pred,labels=[0,1]).reshape(-1)\n",
    "print('Outcome values : \\n', tp, fn, fp, tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
